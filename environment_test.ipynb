{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import OsuEnvironment\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "if not os.path.exists('yolov5'):\n",
    "    !git clone https://github.com/ultralytics/yolov5\n",
    "    !pip install -r yolov5/requirements.txt\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "logging.getLogger('ultralytics').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\bohui/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-11-12 Python-3.11.5 torch-2.5.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# initialize the environment\n",
    "env = OsuEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "tensor([[[  4.],\n",
      "         [ 16.],\n",
      "         [111.],\n",
      "         [210.]],\n",
      "\n",
      "        [[ 14.],\n",
      "         [ 12.],\n",
      "         [  7.],\n",
      "         [ 19.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Shape: (batch_size, key_size, action_size) -> (2, 3, 4)\n",
    "q_values = torch.tensor([\n",
    "    [[1.0, 2.0, 3.0, 4.0],   # Key 1 (4 action values)\n",
    "     [5.0, 16.0, 7.0, 8.0],   # Key 2 (4 action values)\n",
    "     [9.0, 10.0, 111.0, 12.0],\n",
    "     [9.0, 210.0, 11.0, 12.0]],  # Key 3 (4 action values)\n",
    "    \n",
    "    [[2.0, 14.0, 6.0, 8.0],   # Key 1 (4 action values)\n",
    "     [3.0, 6.0, 9.0, 12.0],   # Key 2 (4 action values)\n",
    "     [1.0, 3.0, 5.0, 7.0],\n",
    "     [19.0, 10.0, 11.0, 12.0]]   # Key 3 (4 action values)\n",
    "])\n",
    "\n",
    "action = q_values.argmax(dim=2)\n",
    "print(action.shape)\n",
    "print(q_values.gather(2, action.unsqueeze(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "from gymnasium import spaces\n",
    "action_space = spaces.MultiDiscrete([4]*4)\n",
    "\n",
    "print(action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening on 127.0.0.1:5555\n",
      "Connection from ('127.0.0.1', 62958)\n",
      "Episode: 0, State: []\n",
      "Action: [3 1 1 2], Reward: 0\n",
      "It takes 0.12340116500854492 to finish one step method\n",
      "Episode: 1, State: []\n",
      "Action: [3 1 0 0], Reward: 0\n",
      "It takes 0.09153485298156738 to finish one step method\n",
      "Episode: 2, State: []\n",
      "Action: [2 1 2 0], Reward: 0\n",
      "It takes 0.09225177764892578 to finish one step method\n",
      "Episode: 3, State: []\n",
      "Action: [1 3 0 0], Reward: 0\n",
      "It takes 0.04597043991088867 to finish one step method\n",
      "Episode: 4, State: []\n",
      "Action: [1 2 3 2], Reward: 0\n",
      "It takes 0.09146690368652344 to finish one step method\n",
      "Episode: 5, State: []\n",
      "Action: [0 2 0 3], Reward: 0\n",
      "It takes 0.09126877784729004 to finish one step method\n",
      "Episode: 6, State: []\n",
      "Action: [1 2 0 3], Reward: 0\n",
      "It takes 0.046027421951293945 to finish one step method\n",
      "Episode: 7, State: []\n",
      "Action: [3 3 2 2], Reward: 0\n",
      "It takes 0.04648613929748535 to finish one step method\n",
      "Episode: 8, State: []\n",
      "Action: [0 1 2 3], Reward: 0\n",
      "It takes 0.046315908432006836 to finish one step method\n",
      "Episode: 9, State: []\n",
      "Action: [1 1 1 3], Reward: 0\n",
      "It takes 0.04542660713195801 to finish one step method\n",
      "Episode: 10, State: []\n",
      "Action: [1 2 3 2], Reward: 0\n",
      "It takes 0.045716047286987305 to finish one step method\n",
      "Episode: 11, State: []\n",
      "Action: [0 1 3 2], Reward: 0\n",
      "It takes 0.04601120948791504 to finish one step method\n",
      "Episode: 12, State: []\n",
      "Action: [2 3 2 0], Reward: 0\n",
      "It takes 0.04596853256225586 to finish one step method\n",
      "Episode: 13, State: []\n",
      "Action: [1 3 3 3], Reward: 0\n",
      "It takes 0.04665684700012207 to finish one step method\n",
      "Episode: 14, State: []\n",
      "Action: [2 0 2 1], Reward: 0\n",
      "It takes 0.04546976089477539 to finish one step method\n",
      "Episode: 15, State: []\n",
      "Action: [3 2 0 3], Reward: 0\n",
      "It takes 0.045041799545288086 to finish one step method\n",
      "Episode: 16, State: []\n",
      "Action: [0 1 0 0], Reward: 0\n",
      "It takes 0.04607820510864258 to finish one step method\n",
      "Episode: 17, State: []\n",
      "Action: [2 0 1 1], Reward: 0\n",
      "It takes 0.09215140342712402 to finish one step method\n",
      "Episode: 18, State: []\n",
      "Action: [3 2 1 3], Reward: 0\n",
      "It takes 0.04543185234069824 to finish one step method\n",
      "Episode: 19, State: []\n",
      "Action: [3 2 2 2], Reward: 0\n",
      "It takes 0.04621386528015137 to finish one step method\n",
      "Episode: 20, State: []\n",
      "Action: [1 2 1 0], Reward: 0\n",
      "It takes 0.045479774475097656 to finish one step method\n",
      "Episode: 21, State: []\n",
      "Action: [2 1 2 0], Reward: 0\n",
      "It takes 0.04545450210571289 to finish one step method\n",
      "Episode: 22, State: []\n",
      "Action: [2 3 3 2], Reward: 0\n",
      "It takes 0.04665017127990723 to finish one step method\n",
      "Episode: 23, State: []\n",
      "Action: [1 2 0 0], Reward: 0\n",
      "It takes 0.045307159423828125 to finish one step method\n",
      "Episode: 24, State: []\n",
      "Action: [0 1 0 0], Reward: 0\n",
      "It takes 0.0465390682220459 to finish one step method\n",
      "Episode: 25, State: []\n",
      "Action: [1 2 3 0], Reward: 0\n",
      "It takes 0.046106815338134766 to finish one step method\n",
      "Episode: 26, State: []\n",
      "Action: [0 0 0 2], Reward: 0\n",
      "It takes 0.045652151107788086 to finish one step method\n",
      "Episode: 27, State: []\n",
      "Action: [0 0 0 0], Reward: 0\n",
      "It takes 0.04654741287231445 to finish one step method\n",
      "Episode: 28, State: []\n",
      "Action: [1 1 1 1], Reward: 0\n",
      "It takes 0.04603934288024902 to finish one step method\n",
      "Episode: 29, State: []\n",
      "Action: [1 2 2 2], Reward: 0\n",
      "It takes 0.04590892791748047 to finish one step method\n",
      "Episode: 30, State: []\n",
      "Action: [1 1 0 0], Reward: 0\n",
      "It takes 0.04640507698059082 to finish one step method\n",
      "Episode: 31, State: []\n",
      "Action: [0 1 0 1], Reward: 0\n",
      "It takes 0.045838117599487305 to finish one step method\n",
      "Episode: 32, State: []\n",
      "Action: [2 2 3 3], Reward: 0\n",
      "It takes 0.04566645622253418 to finish one step method\n",
      "Episode: 33, State: []\n",
      "Action: [0 1 0 0], Reward: 0\n",
      "It takes 0.04618358612060547 to finish one step method\n",
      "Episode: 34, State: []\n",
      "Action: [2 0 3 0], Reward: 0\n",
      "It takes 0.04631495475769043 to finish one step method\n",
      "Episode: 35, State: []\n",
      "Action: [2 3 0 1], Reward: 0\n",
      "It takes 0.046439170837402344 to finish one step method\n",
      "Episode: 36, State: []\n",
      "Action: [1 2 3 3], Reward: 0\n",
      "It takes 0.04628729820251465 to finish one step method\n",
      "Episode: 37, State: []\n",
      "Action: [3 1 1 1], Reward: 0\n",
      "It takes 0.04654574394226074 to finish one step method\n",
      "Episode: 38, State: []\n",
      "Action: [1 1 0 2], Reward: 0\n",
      "It takes 0.04577922821044922 to finish one step method\n",
      "Episode: 39, State: []\n",
      "Action: [1 3 1 2], Reward: 0\n",
      "It takes 0.04684615135192871 to finish one step method\n",
      "Episode: 40, State: []\n",
      "Action: [2 0 3 2], Reward: 0\n",
      "It takes 0.04563736915588379 to finish one step method\n",
      "Episode: 41, State: []\n",
      "Action: [0 1 3 2], Reward: 0\n",
      "It takes 0.04564476013183594 to finish one step method\n",
      "Episode: 42, State: []\n",
      "Action: [0 1 2 0], Reward: 0\n",
      "It takes 0.04508614540100098 to finish one step method\n",
      "Episode: 43, State: [[1, 1, 17]]\n",
      "Action: [1 2 1 1], Reward: 0\n",
      "It takes 0.1062161922454834 to finish one step method\n",
      "Episode: 44, State: [[1, 1, 51]]\n",
      "Action: [3 0 1 0], Reward: 0\n",
      "It takes 0.046019792556762695 to finish one step method\n",
      "Episode: 45, State: [[1, 1, 64], [1, 2, 14]]\n",
      "Action: [3 2 1 1], Reward: 0\n",
      "It takes 0.045744895935058594 to finish one step method\n",
      "Episode: 46, State: [[1, 1, 88], [1, 2, 29]]\n",
      "Action: [3 1 3 3], Reward: 0\n",
      "It takes 0.04624152183532715 to finish one step method\n",
      "Episode: 47, State: [[1, 1, 115], [1, 2, 43]]\n",
      "Action: [3 0 1 3], Reward: 0\n",
      "It takes 0.04604339599609375 to finish one step method\n",
      "Episode: 48, State: [[1, 1, 147], [1, 2, 59]]\n",
      "Action: [3 3 3 0], Reward: 0\n",
      "It takes 0.04607820510864258 to finish one step method\n",
      "Episode: 49, State: [[1, 1, 172], [1, 2, 72]]\n",
      "Action: [2 0 2 2], Reward: 0\n",
      "It takes 0.04622483253479004 to finish one step method\n",
      "Episode: 50, State: [[1, 1, 204], [1, 2, 101]]\n",
      "Action: [2 3 3 3], Reward: 0\n",
      "It takes 0.045563459396362305 to finish one step method\n",
      "Episode: 51, State: [[1, 1, 231], [1, 2, 130]]\n",
      "Action: [1 2 3 2], Reward: 0\n",
      "It takes 0.046547889709472656 to finish one step method\n",
      "Episode: 52, State: [[1, 1, 261], [1, 2, 158], [1, 2, 13]]\n",
      "Action: [0 3 2 3], Reward: 0\n",
      "It takes 0.04634356498718262 to finish one step method\n",
      "Episode: 53, State: [[1, 1, 290], [1, 2, 188], [1, 2, 29]]\n",
      "Action: [0 1 1 3], Reward: 0\n",
      "It takes 0.06132769584655762 to finish one step method\n",
      "Episode: 54, State: [[1, 1, 330], [1, 2, 49], [1, 2, 228]]\n",
      "Action: [1 2 3 3], Reward: 0\n",
      "It takes 0.04558396339416504 to finish one step method\n",
      "Episode: 55, State: [[1, 1, 357], [1, 2, 62], [1, 2, 253], [1, 1, 10]]\n",
      "Action: [3 0 2 0], Reward: 0\n",
      "It takes 0.04544472694396973 to finish one step method\n",
      "Episode: 56, State: [[1, 1, 24], [1, 1, 386], [1, 2, 77], [1, 2, 284]]\n",
      "Action: [2 2 0 0], Reward: 0\n",
      "It takes 0.04658937454223633 to finish one step method\n",
      "Episode: 57, State: [[1, 1, 415], [1, 1, 40], [1, 2, 311], [1, 2, 107]]\n",
      "Action: [1 2 3 3], Reward: 0\n",
      "It takes 0.045386314392089844 to finish one step method\n",
      "Episode: 58, State: [[1, 1, 43], [1, 1, 422], [1, 2, 115], [1, 2, 320]]\n",
      "Action: [3 0 1 1], Reward: 0\n",
      "It takes 0.046446800231933594 to finish one step method\n",
      "Episode: 59, State: [[1, 1, 451], [1, 1, 57], [1, 2, 347], [1, 2, 143], [1, 2, 47]]\n",
      "Action: [0 2 2 0], Reward: 0\n",
      "It takes 0.04595470428466797 to finish one step method\n",
      "Episode: 60, State: [[1, 1, 72], [1, 1, 477], [1, 2, 374], [1, 2, 170], [1, 2, 60]]\n",
      "Action: [2 1 2 2], Reward: 0\n",
      "It takes 0.04591488838195801 to finish one step method\n",
      "Episode: 61, State: [[1, 1, 508], [1, 1, 100], [1, 2, 405], [1, 2, 201], [1, 2, 87]]\n",
      "Action: [2 3 1 3], Reward: 0\n",
      "It takes 0.04619193077087402 to finish one step method\n",
      "Episode: 62, State: [[1, 1, 130], [1, 1, 536], [1, 2, 434], [1, 2, 229], [1, 2, 117]]\n",
      "Action: [3 0 0 1], Reward: 0\n",
      "It takes 0.060408592224121094 to finish one step method\n",
      "Episode: 63, State: [[1, 1, 578], [1, 1, 171], [1, 2, 272], [1, 2, 476], [1, 2, 161], [1, 2, 19]]\n",
      "Action: [1 1 2 0], Reward: 0\n",
      "It takes 0.0615382194519043 to finish one step method\n",
      "Episode: 64, State: [[1, 1, 616], [1, 1, 209], [1, 2, 39], [1, 2, 310], [1, 2, 514], [1, 2, 197]]\n",
      "Action: [1 2 2 3], Reward: 0\n",
      "It takes 0.046772003173828125 to finish one step method\n",
      "Episode: 65, State: [[1, 1, 641], [1, 1, 232], [1, 2, 51], [1, 2, 538], [1, 2, 334], [1, 2, 221]]\n",
      "Action: [0 0 2 2], Reward: 0\n",
      "It takes 0.04695630073547363 to finish one step method\n",
      "Episode: 66, State: [[1, 2, 67], [1, 1, 671], [1, 1, 264], [1, 2, 569], [1, 2, 365], [1, 2, 253]]\n",
      "Action: [2 1 2 1], Reward: 0\n",
      "It takes 0.04589128494262695 to finish one step method\n",
      "Episode: 67, State: [[1, 1, 700], [1, 1, 295], [1, 2, 600], [1, 2, 90], [1, 2, 282], [1, 2, 396]]\n",
      "Action: [2 2 1 2], Reward: 0\n",
      "It takes 0.045786142349243164 to finish one step method\n",
      "Episode: 68, State: [[1, 1, 712], [1, 1, 321], [1, 2, 117], [1, 2, 627], [1, 2, 422], [1, 2, 310]]\n",
      "Action: [2 1 2 2], Reward: 0\n",
      "It takes 0.046099185943603516 to finish one step method\n",
      "Episode: 69, State: [[1, 1, 352], [1, 1, 727], [1, 2, 657], [1, 2, 146], [1, 2, 453], [1, 2, 338]]\n",
      "Action: [2 2 3 0], Reward: 0\n",
      "It takes 0.046205759048461914 to finish one step method\n",
      "Episode: 70, State: [[1, 1, 743], [1, 1, 381], [1, 2, 687], [1, 2, 176], [1, 2, 483], [1, 2, 370]]\n",
      "Action: [1 0 2 2], Reward: 0\n",
      "It takes 0.04657435417175293 to finish one step method\n",
      "Episode: 71, State: [[1, 1, 409], [1, 2, 204], [1, 2, 703], [1, 2, 395], [1, 2, 510], [1, 1, 756]]\n",
      "Action: [2 0 2 1], Reward: 0\n",
      "It takes 0.045278310775756836 to finish one step method\n",
      "Episode: 72, State: [[1, 1, 439], [1, 2, 234], [1, 2, 540], [1, 2, 718], [1, 2, 430]]\n",
      "Action: [1 0 1 3], Reward: 0\n",
      "It takes 0.04558229446411133 to finish one step method\n",
      "Episode: 73, State: [[1, 1, 467], [1, 2, 260], [1, 2, 733], [1, 2, 567], [1, 1, 14], [1, 2, 453]]\n",
      "Action: [1 1 1 2], Reward: -11\n",
      "It takes 0.053981781005859375 to finish one step method\n",
      "Episode: 74, State: [[1, 1, 31], [1, 1, 500], [1, 2, 295], [1, 2, 602], [1, 2, 489], [1, 2, 751]]\n",
      "Action: [1 2 3 2], Reward: 0\n",
      "It takes 0.06890130043029785 to finish one step method\n",
      "Episode: 75, State: [[1, 1, 550], [1, 1, 55], [1, 2, 343], [1, 2, 650], [1, 2, 536]]\n",
      "Action: [1 2 2 3], Reward: -8\n",
      "It takes 0.04139137268066406 to finish one step method\n",
      "Episode: 76, State: [[1, 1, 65], [1, 1, 570], [1, 2, 364], [1, 2, 671], [1, 2, 557], [1, 2, 14]]\n",
      "Action: [0 2 0 2], Reward: 0\n",
      "It takes 0.06441712379455566 to finish one step method\n",
      "Episode: 77, State: [[1, 1, 613], [1, 1, 102], [1, 2, 37], [1, 2, 407], [1, 2, 704], [1, 2, 601]]\n",
      "Action: [2 2 1 2], Reward: 0\n",
      "It takes 0.07643914222717285 to finish one step method\n",
      "Episode: 78, State: [[1, 1, 664], [1, 2, 62], [1, 1, 152], [1, 2, 458], [1, 2, 729], [1, 1, 48], [1, 2, 650]]\n",
      "Action: [2 2 3 2], Reward: -8\n",
      "It takes 0.040894508361816406 to finish one step method\n",
      "Episode: 79, State: [[1, 1, 684], [1, 2, 72], [1, 1, 172], [1, 1, 59], [1, 2, 478], [1, 2, 740], [1, 2, 675]]\n",
      "Action: [1 2 2 3], Reward: -9\n",
      "It takes 0.05277681350708008 to finish one step method\n",
      "Episode: 80, State: [[1, 1, 705], [1, 1, 208], [1, 2, 105], [1, 1, 93], [1, 2, 513], [1, 2, 703]]\n",
      "Action: [3 0 3 0], Reward: -12\n",
      "It takes 0.039682865142822266 to finish one step method\n",
      "Episode: 81, State: [[1, 1, 717], [1, 1, 231], [1, 1, 117], [1, 2, 537], [1, 2, 129], [1, 2, 716]]\n",
      "Action: [3 1 3 0], Reward: 0\n",
      "It takes 0.050662994384765625 to finish one step method\n",
      "Episode: 82, State: [[1, 1, 735], [1, 2, 163], [1, 1, 265], [1, 2, 570], [1, 2, 735], [1, 1, 150], [1, 2, 16]]\n",
      "Action: [0 1 1 3], Reward: 0\n",
      "It takes 0.046143293380737305 to finish one step method\n",
      "Episode: 83, State: [[1, 1, 185], [1, 1, 295], [1, 2, 193], [1, 2, 32], [1, 2, 600], [1, 1, 751], [1, 2, 750]]\n",
      "Action: [2 3 2 3], Reward: -13\n",
      "It takes 0.053943634033203125 to finish one step method\n",
      "Episode: 84, State: [[1, 1, 328], [1, 2, 225], [1, 2, 48], [1, 1, 215], [1, 2, 634]]\n",
      "Action: [2 2 3 3], Reward: -7\n",
      "It takes 0.042751312255859375 to finish one step method\n",
      "Episode: 85, State: [[1, 1, 353], [1, 1, 238], [1, 2, 659], [1, 2, 250], [1, 2, 61]]\n",
      "Action: [2 0 1 0], Reward: 0\n",
      "It takes 0.04268217086791992 to finish one step method\n",
      "Episode: 86, State: [[1, 1, 385], [1, 2, 78], [1, 1, 270], [1, 2, 689], [1, 2, 282]]\n",
      "Action: [3 0 2 1], Reward: 0\n",
      "It takes 0.04583287239074707 to finish one step method\n",
      "Episode: 87, State: [[1, 1, 299], [1, 2, 102], [1, 1, 409], [1, 2, 306], [1, 2, 703]]\n",
      "Action: [1 1 3 2], Reward: -8\n",
      "It takes 0.03779935836791992 to finish one step method\n",
      "Episode: 88, State: [[1, 1, 435], [1, 1, 319], [1, 2, 332], [1, 2, 128], [1, 2, 715]]\n",
      "Action: [1 1 1 1], Reward: -13\n",
      "It takes 0.03303241729736328 to finish one step method\n",
      "Episode: 89, State: [[1, 1, 456], [1, 2, 352], [1, 2, 148], [1, 1, 340], [1, 2, 726]]\n",
      "Action: [2 1 3 2], Reward: 0\n",
      "It takes 0.052245378494262695 to finish one step method\n",
      "Episode: 90, State: [[1, 1, 377], [1, 2, 181], [1, 1, 488], [1, 2, 386], [1, 2, 744]]\n",
      "Action: [3 2 1 2], Reward: 0\n",
      "It takes 0.060608863830566406 to finish one step method\n",
      "Episode: 91, State: [[1, 2, 223], [1, 2, 427], [1, 1, 531], [1, 1, 415]]\n",
      "Action: [2 2 1 0], Reward: 0\n",
      "It takes 0.04564023017883301 to finish one step method\n",
      "Episode: 92, State: [[1, 2, 452], [1, 1, 555], [1, 1, 441], [1, 2, 249]]\n",
      "Action: [3 2 3 3], Reward: 0\n",
      "It takes 0.04622197151184082 to finish one step method\n",
      "Episode: 93, State: [[1, 1, 23], [1, 1, 586], [1, 2, 280], [1, 1, 470], [1, 2, 483]]\n",
      "Action: [3 1 0 2], Reward: 0\n",
      "It takes 0.045014142990112305 to finish one step method\n",
      "Episode: 94, State: [[1, 1, 35], [1, 2, 510], [1, 1, 612], [1, 2, 305], [1, 1, 498]]\n",
      "Action: [1 3 2 0], Reward: -8\n",
      "It takes 0.04078102111816406 to finish one step method\n",
      "Episode: 95, State: [[1, 1, 47], [1, 1, 636], [1, 2, 533], [1, 1, 523], [1, 2, 330]]\n",
      "Action: [1 3 3 1], Reward: 0\n",
      "It takes 0.0520017147064209 to finish one step method\n",
      "Episode: 96, State: [[1, 1, 67], [1, 1, 675], [1, 2, 572], [1, 1, 562], [1, 2, 368]]\n",
      "Action: [3 0 2 2], Reward: 0\n",
      "It takes 0.045950889587402344 to finish one step method\n",
      "Episode: 97, State: [[1, 1, 87], [1, 1, 697], [1, 2, 394], [1, 1, 585], [1, 2, 597]]\n",
      "Action: [3 3 0 3], Reward: 0\n",
      "It takes 0.07665205001831055 to finish one step method\n",
      "Episode: 98, State: [[1, 1, 722], [1, 2, 649], [1, 1, 638], [1, 1, 138], [1, 2, 445], [1, 1, 43]]\n",
      "Action: [3 3 3 2], Reward: -8\n",
      "It takes 0.04296612739562988 to finish one step method\n",
      "Episode: 99, State: [[1, 1, 735], [1, 1, 161], [1, 2, 468], [1, 1, 664], [1, 1, 54], [1, 2, 671], [1, 2, 15]]\n",
      "Action: [1 0 0 0], Reward: 0\n",
      "It takes 0.04839444160461426 to finish one step method\n",
      "Episode: 100, State: [[1, 1, 194], [1, 1, 697], [1, 2, 500], [1, 2, 699], [1, 2, 32], [1, 1, 79], [1, 1, 751]]\n",
      "Action: [2 1 3 2], Reward: 0\n",
      "It takes 0.04593348503112793 to finish one step method\n",
      "Episode: 101, State: [[1, 1, 712], [1, 1, 222], [1, 2, 526], [1, 2, 45], [1, 1, 106], [1, 2, 711]]\n",
      "Action: [0 0 2 1], Reward: -7\n",
      "It takes 0.04173541069030762 to finish one step method\n",
      "Episode: 102, State: [[1, 1, 725], [1, 1, 247], [1, 2, 725], [1, 2, 554], [1, 1, 132], [1, 2, 58]]\n",
      "Action: [2 3 0 2], Reward: 0\n",
      "It takes 0.05052661895751953 to finish one step method\n",
      "Episode: 103, State: [[1, 1, 743], [1, 1, 22], [1, 1, 280], [1, 2, 586], [1, 2, 743], [1, 2, 75], [1, 1, 163]]\n",
      "Action: [3 1 3 3], Reward: 0\n",
      "It takes 0.04611682891845703 to finish one step method\n",
      "Episode: 104, State: [[1, 1, 308], [1, 1, 37], [1, 2, 615], [1, 2, 103], [1, 1, 193], [1, 1, 757], [1, 2, 757]]\n",
      "Action: [2 3 0 0], Reward: -8\n",
      "It takes 0.036879777908325195 to finish one step method\n",
      "Episode: 105, State: [[1, 1, 48], [1, 2, 638], [1, 1, 331], [1, 2, 126], [1, 1, 217]]\n",
      "Action: [2 0 1 0], Reward: 0\n",
      "It takes 0.054238080978393555 to finish one step method\n",
      "Episode: 106, State: [[1, 1, 65], [1, 1, 365], [1, 2, 160], [1, 2, 673], [1, 1, 252], [1, 2, 14]]\n",
      "Action: [1 3 1 0], Reward: 0\n",
      "It takes 0.04523777961730957 to finish one step method\n",
      "Episode: 107, State: [[1, 1, 89], [1, 1, 395], [1, 2, 697], [1, 2, 29], [1, 1, 278], [1, 2, 189]]\n",
      "Action: [2 3 2 1], Reward: 0\n",
      "It takes 0.04554104804992676 to finish one step method\n",
      "Episode: 108, State: [[1, 1, 116], [1, 1, 423], [1, 2, 44], [1, 2, 712], [1, 2, 218], [1, 1, 310]]\n",
      "Action: [0 0 0 3], Reward: 0\n",
      "It takes 0.04700422286987305 to finish one step method\n",
      "Episode: 109, State: [[1, 1, 149], [1, 1, 455], [1, 2, 727], [1, 1, 339], [1, 2, 249], [1, 2, 59], [1, 1, 8]]\n",
      "Action: [2 2 0 1], Reward: -9\n",
      "It takes 0.03495025634765625 to finish one step method\n",
      "Episode: 110, State: [[1, 1, 168], [1, 1, 475], [1, 2, 70], [1, 1, 358], [1, 2, 738], [1, 2, 268], [1, 1, 18]]\n",
      "Action: [0 0 1 2], Reward: 0\n",
      "It takes 0.0405726432800293 to finish one step method\n",
      "Episode: 111, State: [[1, 1, 30], [1, 1, 193], [1, 1, 500], [1, 2, 87], [1, 1, 385], [1, 2, 293], [1, 2, 750]]\n",
      "Action: [2 1 1 0], Reward: -13\n",
      "It takes 0.04453611373901367 to finish one step method\n",
      "Episode: 112, State: [[1, 1, 44], [1, 1, 222], [1, 1, 529], [1, 1, 413], [1, 2, 117], [1, 2, 323], [1, 2, 35]]\n",
      "Action: [1 3 1 2], Reward: 0\n",
      "It takes 0.0470123291015625 to finish one step method\n",
      "Episode: 113, State: [[1, 1, 252], [1, 1, 60], [1, 1, 558], [1, 2, 353], [1, 1, 445], [1, 2, 148], [1, 2, 49]]\n",
      "Action: [1 3 3 0], Reward: 0\n",
      "It takes 0.045960426330566406 to finish one step method\n",
      "Episode: 114, State: [[1, 1, 282], [1, 1, 588], [1, 1, 76], [1, 1, 472], [1, 2, 382], [1, 2, 177], [1, 2, 65]]\n",
      "Action: [1 0 1 0], Reward: -12\n",
      "It takes 0.04778885841369629 to finish one step method\n",
      "Episode: 115, State: [[1, 1, 107], [1, 1, 310], [1, 1, 617], [1, 1, 503], [1, 2, 413], [1, 2, 208], [1, 2, 95]]\n",
      "Action: [0 2 0 0], Reward: -8\n",
      "It takes 0.03743553161621094 to finish one step method\n",
      "Episode: 116, State: [[1, 1, 335], [1, 1, 130], [1, 1, 641], [1, 1, 530], [1, 2, 231], [1, 2, 435], [1, 2, 119]]\n",
      "Action: [1 1 2 2], Reward: -7\n",
      "It takes 0.034584999084472656 to finish one step method\n",
      "Episode: 117, State: [[1, 1, 357], [1, 1, 151], [1, 1, 663], [1, 1, 547], [1, 2, 457], [1, 2, 252], [1, 2, 141], [1, 2, 10]]\n",
      "Action: [1 1 0 2], Reward: 0\n",
      "It takes 0.04867196083068848 to finish one step method\n",
      "Episode: 118, State: [[1, 1, 184], [1, 1, 387], [1, 1, 693], [1, 2, 26], [1, 2, 489], [1, 1, 579], [1, 2, 284], [1, 2, 170]]\n",
      "Action: [3 3 0 1], Reward: 0\n",
      "It takes 0.04633688926696777 to finish one step method\n",
      "Episode: 119, State: [[1, 1, 211], [1, 1, 417], [1, 2, 40], [1, 1, 709], [1, 2, 312], [1, 2, 202], [1, 1, 607], [1, 2, 518]]\n",
      "Action: [2 2 2 1], Reward: 0\n",
      "It takes 0.0458979606628418 to finish one step method\n",
      "Episode: 120, State: [[1, 1, 241], [1, 1, 722], [1, 1, 444], [1, 2, 546], [1, 2, 55], [1, 1, 637], [1, 2, 341], [1, 2, 227]]\n",
      "Action: [2 2 3 3], Reward: 0\n",
      "It takes 0.04597735404968262 to finish one step method\n",
      "Episode: 121, State: [[1, 1, 739], [1, 1, 270], [1, 2, 71], [1, 1, 476], [1, 2, 576], [1, 1, 668], [1, 2, 372], [1, 2, 259]]\n",
      "Action: [3 0 3 0], Reward: 0\n",
      "It takes 0.04630899429321289 to finish one step method\n",
      "Episode: 122, State: [[1, 1, 300], [1, 1, 702], [1, 1, 505], [1, 2, 606], [1, 2, 96], [1, 2, 401], [1, 2, 288], [1, 1, 753]]\n",
      "Action: [2 2 3 3], Reward: -13\n",
      "It takes 0.04130744934082031 to finish one step method\n",
      "Episode: 123, State: [[1, 1, 529], [1, 1, 323], [1, 1, 713], [1, 2, 119], [1, 2, 424], [1, 2, 629], [1, 2, 314]]\n",
      "Action: [3 2 1 3], Reward: -7\n",
      "It takes 0.033429622650146484 to finish one step method\n",
      "Episode: 124, State: [[1, 1, 723], [1, 1, 345], [1, 1, 551], [1, 2, 140], [1, 2, 652], [1, 2, 447], [1, 2, 337]]\n",
      "Action: [3 2 2 3], Reward: 0\n",
      "It takes 0.047139883041381836 to finish one step method\n",
      "Episode: 125, State: [[1, 1, 741], [1, 1, 376], [1, 1, 580], [1, 2, 682], [1, 2, 171], [1, 2, 477], [1, 2, 364]]\n",
      "Action: [0 2 3 1], Reward: -12\n",
      "It takes 0.06467866897583008 to finish one step method\n",
      "Episode: 126, State: [[1, 1, 416], [1, 1, 621], [1, 2, 210], [1, 2, 707], [1, 2, 517], [1, 2, 404]]\n",
      "Action: [3 0 1 3], Reward: 0\n",
      "It takes 0.05687212944030762 to finish one step method\n",
      "Episode: 127, State: [[1, 1, 451], [1, 1, 654], [1, 2, 724], [1, 2, 245], [1, 2, 552], [1, 2, 438]]\n",
      "Action: [2 2 1 1], Reward: -12\n",
      "It takes 0.03166365623474121 to finish one step method\n",
      "Episode: 128, State: [[1, 1, 472], [1, 1, 675], [1, 2, 266], [1, 2, 572], [1, 2, 736], [1, 2, 458], [1, 2, 17]]\n",
      "Action: [0 1 2 1], Reward: 0\n",
      "It takes 0.04530906677246094 to finish one step method\n",
      "Episode: 129, State: [[1, 1, 700], [1, 1, 499], [1, 2, 32], [1, 2, 294], [1, 2, 601], [1, 2, 488], [1, 2, 750]]\n",
      "Action: [3 3 1 1], Reward: 0\n",
      "It takes 0.046844482421875 to finish one step method\n",
      "Episode: 130, State: [[1, 1, 530], [1, 1, 714], [1, 2, 47], [1, 2, 325], [1, 2, 632], [1, 2, 518]]\n",
      "Action: [2 2 3 2], Reward: -7\n",
      "It takes 0.03688788414001465 to finish one step method\n",
      "Episode: 131, State: [[1, 1, 724], [1, 1, 552], [1, 2, 347], [1, 2, 653], [1, 2, 58], [1, 2, 542]]\n",
      "Action: [2 1 1 3], Reward: 0\n",
      "It takes 0.06962370872497559 to finish one step method\n",
      "Episode: 132, State: [[1, 1, 28], [1, 1, 596], [1, 2, 83], [1, 1, 748], [1, 2, 392], [1, 2, 584], [1, 2, 694]]\n",
      "Action: [0 3 2 2], Reward: 0\n",
      "It takes 0.04676246643066406 to finish one step method\n",
      "Episode: 133, State: [[1, 1, 42], [1, 1, 626], [1, 2, 421], [1, 2, 112], [1, 2, 614], [1, 2, 710]]\n",
      "Action: [0 2 1 3], Reward: 0\n",
      "It takes 0.0457763671875 to finish one step method\n",
      "Episode: 134, State: [[1, 1, 655], [1, 1, 58], [1, 2, 451], [1, 2, 144], [1, 2, 643], [1, 2, 724], [1, 2, 47]]\n",
      "Action: [0 2 0 3], Reward: 0\n",
      "It takes 0.04626107215881348 to finish one step method\n",
      "Episode: 135, State: [[1, 1, 685], [1, 1, 73], [1, 2, 480], [1, 2, 713], [1, 2, 173], [1, 2, 741], [1, 2, 64]]\n",
      "Action: [2 3 0 2], Reward: -9\n",
      "It takes 0.04429268836975098 to finish one step method\n",
      "Episode: 136, State: [[1, 1, 704], [1, 1, 99], [1, 2, 507], [1, 2, 701], [1, 2, 199], [1, 2, 87]]\n",
      "Action: [1 2 1 0], Reward: 0\n",
      "It takes 0.06245088577270508 to finish one step method\n",
      "Episode: 137, State: [[1, 1, 722], [1, 1, 141], [1, 2, 549], [1, 2, 722], [1, 2, 128], [1, 2, 242]]\n",
      "Action: [1 3 1 1], Reward: 0\n",
      "It takes 0.04524660110473633 to finish one step method\n",
      "Episode: 138, State: [[1, 1, 737], [1, 1, 166], [1, 2, 736], [1, 2, 573], [1, 2, 266], [1, 2, 154], [1, 2, 17]]\n",
      "Action: [0 0 3 3], Reward: -9\n",
      "It takes 0.03164482116699219 to finish one step method\n",
      "Episode: 139, State: [[1, 1, 187], [1, 2, 27], [1, 1, 746], [1, 2, 746], [1, 2, 594], [1, 2, 287], [1, 2, 175]]\n",
      "Action: [1 3 3 2], Reward: 0\n",
      "It takes 0.045067787170410156 to finish one step method\n",
      "Episode: 140, State: [[1, 1, 215], [1, 2, 42], [1, 2, 623], [1, 2, 203], [1, 2, 317]]\n",
      "Action: [0 0 0 3], Reward: 0\n",
      "It takes 0.060715675354003906 to finish one step method\n",
      "Episode: 141, State: [[1, 1, 256], [1, 2, 63], [1, 2, 664], [1, 2, 243], [1, 2, 357]]\n",
      "Action: [0 0 0 2], Reward: 0\n",
      "It takes 0.04643607139587402 to finish one step method\n",
      "Episode: 142, State: [[1, 1, 284], [1, 2, 79], [1, 2, 691], [1, 2, 272], [1, 2, 385]]\n",
      "Action: [3 0 3 2], Reward: 0\n",
      "It takes 0.04537343978881836 to finish one step method\n",
      "Episode: 143, State: [[1, 1, 310], [1, 2, 106], [1, 2, 704], [1, 2, 413], [1, 2, 301]]\n",
      "Action: [3 0 0 0], Reward: 0\n",
      "It takes 0.0462031364440918 to finish one step method\n",
      "Episode: 144, State: [[1, 1, 341], [1, 2, 137], [1, 2, 720], [1, 2, 443], [1, 2, 330]]\n",
      "Action: [3 1 1 3], Reward: -8\n",
      "It takes 0.035533905029296875 to finish one step method\n",
      "Episode: 145, State: [[1, 1, 363], [1, 2, 159], [1, 2, 733], [1, 2, 464], [1, 2, 355]]\n",
      "Action: [2 0 0 1], Reward: 0\n",
      "It takes 0.04167747497558594 to finish one step method\n",
      "Episode: 146, State: [[1, 1, 376], [1, 2, 738], [1, 2, 175], [1, 2, 478]]\n",
      "Action: [2 0 2 2], Reward: 0\n",
      "It takes 0.04577827453613281 to finish one step method\n",
      "Episode: 147, State: [[1, 1, 387], [1, 2, 488], [1, 2, 743], [1, 2, 187]]\n",
      "Action: [0 1 1 3], Reward: 0\n",
      "It takes 0.0458219051361084 to finish one step method\n",
      "Episode: 148, State: [[1, 1, 401], [1, 2, 202], [1, 2, 497], [1, 2, 747], [1, 2, 389]]\n",
      "Action: [1 0 1 1], Reward: 0\n",
      "It takes 0.04613924026489258 to finish one step method\n",
      "Episode: 149, State: [[1, 1, 411], [1, 2, 509], [1, 2, 216], [1, 2, 751], [1, 2, 401]]\n",
      "Action: [3 2 3 0], Reward: 0\n",
      "It takes 0.04649949073791504 to finish one step method\n",
      "Episode: 150, State: [[1, 1, 421], [1, 2, 519], [1, 2, 226], [1, 2, 412]]\n",
      "Action: [2 0 0 0], Reward: 0\n",
      "It takes 0.04582691192626953 to finish one step method\n",
      "Episode: 151, State: [[1, 1, 432], [1, 2, 241], [1, 2, 528], [1, 2, 422]]\n",
      "Action: [2 3 0 3], Reward: 0\n",
      "It takes 0.04631686210632324 to finish one step method\n",
      "Episode: 152, State: [[1, 1, 444], [1, 2, 254], [1, 2, 537], [1, 2, 434]]\n",
      "Action: [3 2 2 3], Reward: 0\n",
      "It takes 0.0461580753326416 to finish one step method\n",
      "Episode: 153, State: [[1, 1, 451], [1, 2, 263], [1, 2, 546], [1, 2, 442]]\n",
      "Action: [1 3 1 0], Reward: 0\n",
      "It takes 0.04589653015136719 to finish one step method\n",
      "Episode: 154, State: [[1, 1, 461], [1, 2, 274], [1, 2, 555], [1, 2, 453]]\n",
      "Action: [2 0 0 0], Reward: 0\n",
      "It takes 0.045896291732788086 to finish one step method\n",
      "Episode: 155, State: [[1, 1, 471], [1, 2, 286], [1, 2, 564], [1, 2, 462]]\n",
      "Action: [3 1 2 2], Reward: 0\n",
      "It takes 0.04540419578552246 to finish one step method\n",
      "Episode: 156, State: [[1, 1, 479], [1, 2, 294], [1, 2, 570], [1, 2, 471]]\n",
      "Action: [3 0 1 2], Reward: 0\n",
      "It takes 0.04589581489562988 to finish one step method\n",
      "Episode: 157, State: [[1, 1, 486], [1, 2, 303], [1, 2, 578], [1, 2, 481]]\n",
      "Action: [0 0 0 2], Reward: 0\n",
      "It takes 0.045650482177734375 to finish one step method\n",
      "Episode: 158, State: [[1, 2, 313], [1, 1, 494], [1, 2, 587], [1, 2, 489]]\n",
      "Action: [0 1 3 1], Reward: 0\n",
      "It takes 0.046456336975097656 to finish one step method\n",
      "Episode: 159, State: [[1, 1, 503], [1, 2, 323], [1, 2, 594], [1, 2, 499]]\n",
      "Action: [0 3 1 3], Reward: 0\n",
      "It takes 0.04661130905151367 to finish one step method\n",
      "Episode: 160, State: [[1, 1, 510], [1, 2, 331], [1, 2, 602], [1, 2, 505]]\n",
      "Action: [1 1 2 3], Reward: 0\n",
      "It takes 0.06119871139526367 to finish one step method\n",
      "Episode: 161, State: [[1, 1, 519], [1, 2, 342], [1, 2, 611], [1, 2, 515]]\n",
      "Action: [1 1 3 2], Reward: 0\n",
      "It takes 0.04644918441772461 to finish one step method\n",
      "Episode: 162, State: [[1, 1, 527], [1, 2, 350], [1, 2, 618], [1, 2, 522]]\n",
      "Action: [1 0 0 3], Reward: 0\n",
      "It takes 0.04624176025390625 to finish one step method\n",
      "Episode: 163, State: [[1, 1, 533], [1, 2, 357], [1, 2, 530], [1, 2, 623]]\n",
      "Action: [1 2 0 1], Reward: 0\n",
      "It takes 0.06084942817687988 to finish one step method\n",
      "Episode: 164, State: [[1, 1, 541], [1, 2, 368], [1, 2, 542], [1, 2, 636]]\n",
      "Action: [2 1 0 1], Reward: 0\n",
      "It takes 0.061187028884887695 to finish one step method\n",
      "Episode: 165, State: [[1, 1, 550], [1, 2, 376], [1, 2, 551], [1, 1, 79]]\n",
      "Action: [2 0 1 3], Reward: 0\n",
      "It takes 0.04666638374328613 to finish one step method\n",
      "Episode: 166, State: [[1, 1, 556], [1, 2, 383], [1, 2, 558], [1, 1, 83]]\n",
      "Action: [2 1 3 3], Reward: 0\n",
      "It takes 0.045979976654052734 to finish one step method\n",
      "Episode: 167, State: [[1, 1, 562], [1, 2, 390], [1, 2, 563], [1, 1, 87]]\n",
      "Action: [0 3 3 3], Reward: 0\n",
      "It takes 0.04723095893859863 to finish one step method\n",
      "Episode: 168, State: [[1, 1, 568], [1, 2, 397], [1, 2, 568], [1, 1, 92]]\n",
      "Action: [2 1 0 1], Reward: 0\n",
      "It takes 0.04551124572753906 to finish one step method\n",
      "Episode: 169, State: [[1, 2, 401], [1, 1, 573], [1, 1, 96]]\n",
      "Action: [2 3 0 2], Reward: 0\n",
      "It takes 0.045941829681396484 to finish one step method\n",
      "Episode: 170, State: [[1, 1, 578], [1, 2, 405], [1, 1, 99]]\n",
      "Action: [0 2 1 0], Reward: 0\n",
      "It takes 0.04602456092834473 to finish one step method\n",
      "Episode: 171, State: [[1, 1, 583], [1, 2, 412], [1, 1, 102]]\n",
      "Action: [1 1 0 2], Reward: 0\n",
      "It takes 0.04625868797302246 to finish one step method\n",
      "Episode: 172, State: [[1, 2, 419], [1, 1, 588], [1, 1, 107], [1, 2, 89]]\n",
      "Action: [2 0 0 0], Reward: 0\n",
      "It takes 0.0611417293548584 to finish one step method\n",
      "Episode: 173, State: [[1, 2, 425], [1, 1, 594], [1, 2, 92]]\n",
      "Action: [2 1 3 0], Reward: 0\n",
      "It takes 0.04672694206237793 to finish one step method\n",
      "Episode: 174, State: [[1, 1, 597], [1, 2, 429], [1, 2, 94]]\n",
      "Action: [2 0 0 0], Reward: 0\n",
      "It takes 0.045043230056762695 to finish one step method\n",
      "Episode: 175, State: [[1, 1, 602], [1, 2, 433], [1, 2, 98]]\n",
      "Action: [3 0 0 0], Reward: 0\n",
      "It takes 0.04537796974182129 to finish one step method\n",
      "Episode: 176, State: [[1, 2, 438], [1, 1, 606], [1, 2, 99], [1, 1, 117]]\n",
      "Action: [2 0 1 3], Reward: 0\n",
      "It takes 0.04611825942993164 to finish one step method\n",
      "Episode: 177, State: [[1, 2, 442], [1, 2, 101], [1, 1, 118]]\n",
      "Action: [0 3 3 2], Reward: 0\n",
      "It takes 0.04624748229980469 to finish one step method\n",
      "Episode: 178, State: [[1, 2, 445], [1, 1, 119], [1, 2, 103]]\n",
      "Action: [3 0 3 3], Reward: 0\n",
      "It takes 0.04620504379272461 to finish one step method\n",
      "Episode: 179, State: [[1, 2, 449], [1, 1, 121], [1, 2, 105]]\n",
      "Action: [1 0 0 1], Reward: 0\n",
      "It takes 0.045365333557128906 to finish one step method\n",
      "Episode: 180, State: [[1, 2, 454], [1, 2, 108], [1, 1, 125]]\n",
      "Action: [3 0 1 0], Reward: 0\n",
      "It takes 0.045793771743774414 to finish one step method\n",
      "Episode: 181, State: [[1, 2, 457], [1, 1, 128], [1, 2, 110]]\n",
      "Action: [1 2 3 0], Reward: 0\n",
      "It takes 0.061495065689086914 to finish one step method\n",
      "Episode: 182, State: [[1, 2, 112], [1, 1, 132], [1, 2, 93]]\n",
      "Action: [1 3 3 2], Reward: 0\n",
      "It takes 0.045880794525146484 to finish one step method\n",
      "Episode: 183, State: [[1, 2, 113], [1, 1, 135], [1, 2, 95]]\n",
      "Action: [0 3 3 3], Reward: 0\n",
      "It takes 0.0469365119934082 to finish one step method\n",
      "Episode: 184, State: [[1, 1, 138], [1, 2, 115], [1, 2, 97]]\n",
      "Action: [3 2 2 1], Reward: 0\n",
      "It takes 0.04594731330871582 to finish one step method\n",
      "Episode: 185, State: [[1, 2, 117], [1, 1, 140], [1, 2, 98]]\n",
      "Action: [0 3 1 3], Reward: 0\n",
      "It takes 0.046128034591674805 to finish one step method\n",
      "Episode: 186, State: [[1, 2, 118], [1, 2, 100], [1, 1, 143]]\n",
      "Action: [2 3 3 0], Reward: 0\n",
      "It takes 0.04564499855041504 to finish one step method\n",
      "Episode: 187, State: [[1, 2, 119], [1, 1, 146], [1, 2, 101]]\n",
      "Action: [1 1 0 2], Reward: 0\n",
      "It takes 0.04598402976989746 to finish one step method\n",
      "Episode: 188, State: [[1, 1, 147], [1, 2, 101], [1, 2, 120]]\n",
      "Action: [2 0 2 2], Reward: 0\n",
      "It takes 0.04632377624511719 to finish one step method\n",
      "Episode: 189, State: [[1, 1, 148], [1, 2, 121], [1, 2, 102]]\n",
      "Action: [3 1 3 3], Reward: 0\n",
      "It takes 0.045831918716430664 to finish one step method\n",
      "Episode: 190, State: [[1, 1, 150], [1, 2, 122], [1, 2, 102]]\n",
      "Action: [3 1 0 2], Reward: 0\n",
      "It takes 0.04667806625366211 to finish one step method\n",
      "Episode: 191, State: [[1, 2, 122], [1, 1, 152], [1, 2, 103]]\n",
      "Action: [1 3 3 2], Reward: 0\n",
      "It takes 0.04592084884643555 to finish one step method\n",
      "Episode: 192, State: [[1, 2, 122], [1, 1, 152]]\n",
      "Action: [1 3 0 0], Reward: 0\n",
      "It takes 0.04556083679199219 to finish one step method\n",
      "Episode: 193, State: [[1, 2, 122], [1, 1, 153]]\n",
      "Action: [1 1 3 0], Reward: 0\n",
      "It takes 0.046181440353393555 to finish one step method\n",
      "Episode: 194, State: [[1, 2, 123], [1, 1, 155]]\n",
      "Action: [0 3 1 3], Reward: 0\n",
      "It takes 0.04565620422363281 to finish one step method\n",
      "Episode: 195, State: [[1, 2, 123], [1, 1, 156], [1, 2, 107]]\n",
      "Action: [0 0 0 1], Reward: 0\n",
      "It takes 0.046358585357666016 to finish one step method\n",
      "Episode: 196, State: [[1, 1, 156], [1, 2, 123]]\n",
      "Action: [0 2 1 1], Reward: 0\n",
      "It takes 0.046057701110839844 to finish one step method\n",
      "Episode: 197, State: [[1, 1, 156], [1, 2, 123]]\n",
      "Action: [0 2 0 0], Reward: 0\n",
      "It takes 0.04616737365722656 to finish one step method\n",
      "Episode: 198, State: [[1, 2, 123], [1, 1, 157]]\n",
      "Action: [1 1 2 0], Reward: 0\n",
      "It takes 0.062279701232910156 to finish one step method\n",
      "Episode: 199, State: [[1, 2, 123], [1, 1, 157]]\n",
      "Action: [1 2 0 3], Reward: 0\n",
      "It takes 0.04526805877685547 to finish one step method\n",
      "Episode: 200, State: [[1, 2, 123], [1, 1, 156]]\n",
      "Action: [3 2 1 2], Reward: 0\n",
      "It takes 0.04591703414916992 to finish one step method\n",
      "Episode: 201, State: [[1, 1, 157], [1, 2, 122]]\n",
      "Action: [1 1 1 0], Reward: 0\n",
      "It takes 0.04634571075439453 to finish one step method\n",
      "Episode: 202, State: []\n",
      "Action: [3 3 1 0], Reward: 0\n",
      "It takes 0.04606366157531738 to finish one step method\n",
      "Episode: 203, State: []\n",
      "Action: [2 0 2 3], Reward: 0\n",
      "It takes 0.04676055908203125 to finish one step method\n",
      "Episode: 204, State: []\n",
      "Action: [3 3 1 3], Reward: 0\n",
      "It takes 0.04512429237365723 to finish one step method\n",
      "Episode: 205, State: []\n",
      "Action: [0 1 1 2], Reward: 0\n",
      "It takes 0.04657769203186035 to finish one step method\n",
      "Episode: 206, State: []\n",
      "Action: [1 0 0 3], Reward: 0\n",
      "It takes 0.045419931411743164 to finish one step method\n",
      "Episode: 207, State: []\n",
      "Action: [1 0 1 3], Reward: 0\n",
      "It takes 0.04552173614501953 to finish one step method\n",
      "Episode: 208, State: []\n",
      "Action: [0 1 0 2], Reward: 0\n",
      "It takes 0.046443939208984375 to finish one step method\n",
      "Episode: 209, State: []\n",
      "Action: [3 1 2 1], Reward: 0\n",
      "It takes 0.04610919952392578 to finish one step method\n",
      "Episode: 210, State: []\n",
      "Action: [2 3 2 1], Reward: 0\n",
      "It takes 0.045810699462890625 to finish one step method\n",
      "Episode: 211, State: []\n",
      "Action: [3 2 0 0], Reward: 0\n",
      "It takes 0.04601240158081055 to finish one step method\n",
      "Episode: 212, State: []\n",
      "Action: [3 2 3 2], Reward: 0\n",
      "It takes 0.046541690826416016 to finish one step method\n",
      "Episode: 213, State: []\n",
      "Action: [0 1 1 2], Reward: 0\n",
      "It takes 0.06160402297973633 to finish one step method\n",
      "Episode: 214, State: []\n",
      "Action: [3 2 3 3], Reward: 0\n",
      "It takes 0.04610490798950195 to finish one step method\n",
      "Episode: 215, State: []\n",
      "Action: [0 2 3 0], Reward: 0\n",
      "It takes 0.045825958251953125 to finish one step method\n",
      "Episode: 216, State: []\n",
      "Action: [1 3 3 0], Reward: 0\n",
      "It takes 0.04620695114135742 to finish one step method\n",
      "Episode: 217, State: []\n",
      "Action: [2 1 2 0], Reward: 0\n",
      "It takes 0.045953989028930664 to finish one step method\n",
      "Episode: 218, State: []\n",
      "Action: [1 0 1 1], Reward: 0\n",
      "It takes 0.04663896560668945 to finish one step method\n",
      "Episode: 219, State: []\n",
      "Action: [3 3 0 3], Reward: 0\n",
      "It takes 0.045166730880737305 to finish one step method\n",
      "Episode: 220, State: []\n",
      "Action: [3 3 2 3], Reward: 0\n",
      "It takes 0.04637885093688965 to finish one step method\n",
      "Episode: 221, State: []\n",
      "Action: [2 3 1 1], Reward: 0\n",
      "It takes 0.04665637016296387 to finish one step method\n",
      "Episode: 222, State: []\n",
      "Action: [2 0 3 3], Reward: 0\n",
      "It takes 0.06138968467712402 to finish one step method\n",
      "Episode: 223, State: [[1, 2, 143]]\n",
      "Action: [0 1 0 1], Reward: 0\n",
      "It takes 0.046301841735839844 to finish one step method\n",
      "Episode: 224, State: [[1, 2, 156], [1, 2, 140]]\n",
      "Action: [0 2 1 0], Reward: 0\n",
      "It takes 0.04616189002990723 to finish one step method\n",
      "Episode: 225, State: [[1, 2, 160]]\n",
      "Action: [3 0 0 0], Reward: 0\n",
      "It takes 0.045339107513427734 to finish one step method\n",
      "Connection closed.\n",
      "Episode: 226, State: []\n",
      "Action: [2 3 3 3], Reward: 0\n",
      "It takes 0.04569530487060547 to finish one step method\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# run the environment till song end\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m env\u001b[38;5;241m.\u001b[39mchecking_connection():\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39msong_begin():\n\u001b[0;32m      9\u001b[0m         time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "state = env.reset()\n",
    "done = False\n",
    "count = 0\n",
    "time_total = 0\n",
    "\n",
    "# run the environment till song end\n",
    "while not done and env.checking_connection():\n",
    "    if env.song_begin():\n",
    "        time_start = time.time()\n",
    "        random_action = env.action_space.sample()\n",
    "        state, reward, terminate, truncate = env.step(random_action)\n",
    "        done = terminate or truncate\n",
    "\n",
    "        print(f\"Episode: {count}, State: {state}\")\n",
    "        print(f\"Action: {random_action}, Reward: {reward}\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        time_total += time.time() - time_start\n",
    "        print(f\"It takes {time.time() - time_start} to finish one step method\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
