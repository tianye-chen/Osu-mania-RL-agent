{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from reward import SimulatedReward\n",
    "from helper import detect, capture, pad_inner_array, SocketListener\n",
    "from pynput import keyboard\n",
    "from pynput.keyboard import Controller, Key\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import mss\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('yolov5'):\n",
    "    !git clone https://github.com/ultralytics/yolov5\n",
    "    !pip install -r yolov5/requirements.txt\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "logging.getLogger('ultralytics').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC_Net(nn.Module):\n",
    "  def __init__(self, input, action_space):\n",
    "    super(AC_Net, self).__init__()\n",
    "    self.fc1 = nn.Linear(input[0] * input[1], 128)\n",
    "    self.fc2 = nn.Linear(128, 128)\n",
    "    self.lstm = nn.LSTM(128, 128, batch_first=True)\n",
    "    self.actors = nn.ModuleList([nn.Linear(128, action.n) for action in action_space])\n",
    "    self.critic = nn.Linear(128, 1)\n",
    "    \n",
    "    for layer in [self.fc1, self.fc2, *self.actors, self.critic]:\n",
    "      nn.init.xavier_uniform_(layer.weight)\n",
    "      nn.init.constant_(layer.bias, 0)\n",
    "    \n",
    "  def forward(self, x, hx=None):\n",
    "    x = x.flatten()\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = nn.Dropout(0.3)(x)\n",
    "    x, hx = self.lstm(x.unsqueeze(0), hx)\n",
    "    return [actor(x) for actor in self.actors], self.critic(x), hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimOsuEnvironment(gym.Env):\n",
    "  def __init__(self, max_notes=8, render_mode=False):\n",
    "    self.max_notes = max_notes\n",
    "    self.keys_reference = ['s', 'd', 'k', 'l'] # Used for keyboard input with index corresponding to a lane\n",
    "    self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(self.max_notes, 3))\n",
    "    self.action_space = gym.spaces.MultiDiscrete([4] * len(self.keys_reference))\n",
    "    self.observation = np.zeros((self.max_notes, 3))\n",
    "\n",
    "    self.custom_reward = [50, 100, 2, -1, -15, -2, -2, -150, -1] # Custom reward function to be used with SimulatedReward\n",
    "    self.reward_scale = 100 # Scaling reward to avoid extremely small values\n",
    "    self.custom_reward_scaled = []  # Scaled custom reward, to be set in reset\n",
    "    self.invalid_action_penalty = -5\n",
    "    self.invalid_action_penalty_scaled = 0 # Scaled invalid action penalty, to be set in reset\n",
    "    self.sim_reward = SimulatedReward()\n",
    "    self.executor = ThreadPoolExecutor()\n",
    "    \n",
    "    self.model = None \n",
    "    self.capture_region = None\n",
    "    self._vision_setup()\n",
    "    \n",
    "    self.chosen_folder = None # A randomly chosen folder from available_folders, to be set in reset\n",
    "    self.available_folders = os.listdir('./frames/rl_simulated_training_sample')\n",
    "    self.frames = [] # A list of frames in the chosen folder, to be set in reset\n",
    "    self.frame_notes = [] # A note representation of each frame\n",
    "    self.metadata = {} # Metadata of the chosen song, to be set in reset\n",
    "    self.frame_step = 0\n",
    "    self.note_count = 0\n",
    "    \n",
    "    self.invalid_actions = 0 # Counts the number of invalid actions in a step\n",
    "    self.terminated = False\n",
    "    self.render_mode = render_mode\n",
    "    \n",
    "    self.total_invalid_actions = 0\n",
    "    self.actions_taken = np.zeros((4, 4))\n",
    "    \n",
    "    # Render mode requires the game to be running and the game window to be visible\n",
    "    if self.render_mode:\n",
    "      self.listener = SocketListener()\n",
    "      self.listener.start()\n",
    "      self.keyboard = Controller()\n",
    "    \n",
    "  def reset(self):\n",
    "    self.observation = np.zeros((self.max_notes, 3))\n",
    "    self.invalid_actions = 0\n",
    "    self.total_invalid_actions = 0\n",
    "    self.actions_taken = np.zeros((4, 4))\n",
    "    \n",
    "    if not self.render_mode:\n",
    "      random_folder = random.choice(self.available_folders)\n",
    "      self.chosen_folder = './frames/rl_simulated_training_sample/' + random_folder\n",
    "      self.frames = os.listdir(self.chosen_folder)[:-1]\n",
    "      self.frame_notes = []\n",
    "      self.frame_step = 0\n",
    "      \n",
    "      self.metadata = json.load(open(self.chosen_folder + '/metadata.json'))\n",
    "      self.note_count = self.metadata['note_count'] + self.metadata['hold_note_count'] * 2\n",
    "      \n",
    "      self.custom_reward_scaled = [(r / self.note_count) * self.reward_scale for r in self.custom_reward]\n",
    "      self.invalid_action_penalty_scaled = (self.invalid_action_penalty / self.note_count) * self.reward_scale\n",
    "      self.sim_reward.set_custom_rewards(self.custom_reward_scaled)\n",
    "    \n",
    "    return self.observation\n",
    "  \n",
    "  def step(self, multi_actions):\n",
    "    if self.render_mode:\n",
    "      # Wait for the first connection to be established when starting a song in-game\n",
    "      while self.listener.is_first_connection and not self.listener.has_connection:\n",
    "        pass\n",
    "      \n",
    "      if self.listener.has_connection:\n",
    "        img = capture(self.capture_region)\n",
    "      \n",
    "      # Terminates when the song ends\n",
    "      self.terminated = not self.listener.is_first_connection and not self.listener.has_connection\n",
    "    else:\n",
    "      img = Image.open(self.chosen_folder + '/' + self.frames[self.frame_step])\n",
    "      self.frame_step += 1\n",
    "      self.terminated = self.frame_step >= len(self.frames)\n",
    "    \n",
    "    if img:\n",
    "      vision_thread = self.executor.submit(detect, img, self.model)\n",
    "      self.observation = vision_thread.result()\n",
    "      self.frame_notes.append(self.observation)\n",
    "      \n",
    "    actions, action_types = self._parse_multi_actions(multi_actions)\n",
    "    reward = self.sim_reward.get_simulated_reward(actions, self.frame_notes[-2] if len(self.frame_notes) > 3 else []) + self.invalid_action_penalty_scaled * self.invalid_actions\n",
    "    self.total_invalid_actions += self.invalid_actions\n",
    "    self.invalid_actions = 0\n",
    "    \n",
    "    if len(actions) > 0 and self.render_mode:\n",
    "      self.executor.submit(self._perform_keyboard_action, actions, action_types)\n",
    "    \n",
    "    info = {}\n",
    "    self.observation = pad_inner_array([self.observation], [0, 0 ,0], self.max_notes)[0]\n",
    "    \n",
    "    \n",
    "    return self.observation, reward, self.terminated, info\n",
    "    \n",
    "  def get_meta_data(self):\n",
    "    '''\n",
    "    Returns the metadata of the current song in the form of \n",
    "    {\n",
    "      'song_name' (str), \\\\\n",
    "      'song_duration' (int) in seconds, \\\\\n",
    "      'note_count' (int,) \\\\\n",
    "      'hold_note_count' (int), \\\\\n",
    "      'difficulty' (float)\n",
    "    }\n",
    "    '''\n",
    "    return self.metadata\n",
    "  \n",
    "  def _parse_multi_actions(self, multi_actions):\n",
    "    '''\n",
    "    Parses the multi actions into a list of actions and their types\n",
    "    \n",
    "    '''\n",
    "    parsed_actions = []\n",
    "    action_type = []\n",
    "    \n",
    "    for action_lane, action in enumerate(multi_actions):\n",
    "      match action:\n",
    "        case 0: # Do nothing\n",
    "          self.actions_taken[action_lane][0] += 1\n",
    "        case 1: # Release\n",
    "          self.actions_taken[action_lane][1] += 1\n",
    "          \n",
    "          if not self.render_mode:\n",
    "            if not self.sim_reward.get_key_held(action_lane):\n",
    "              self.invalid_actions += 1\n",
    "              continue\n",
    "            \n",
    "            self.sim_reward.update_keys_held(action_lane, False)\n",
    "          \n",
    "          parsed_actions.append(action_lane)\n",
    "        case 2: # Press\n",
    "          self.actions_taken[action_lane][2] += 1\n",
    "          \n",
    "          if self.sim_reward.get_key_held(action_lane) and not self.render_mode:\n",
    "            self.invalid_actions += 1\n",
    "            \n",
    "          self.sim_reward.update_keys_held(action_lane, False)\n",
    "          parsed_actions.append(action_lane)\n",
    "        case 3: # Hold\n",
    "          self.actions_taken[action_lane][3] += 1\n",
    "          \n",
    "          if not self.sim_reward.get_key_held(action_lane):\n",
    "            parsed_actions.append(action_lane)\n",
    "          \n",
    "          if not self.render_mode:\n",
    "            self.sim_reward.update_keys_held(action_lane, True)\n",
    "\n",
    "      if action_type != 0:\n",
    "        action_type.append(action)\n",
    "          \n",
    "    return parsed_actions, action_type\n",
    "  \n",
    "  def _perform_keyboard_action(self, parsed_actions, action_type):\n",
    "    for action, action_type in zip(parsed_actions, action_type):\n",
    "      if action_type in [2, 3]:\n",
    "        self.keyboard.press(self.keys_reference[action])\n",
    "        if action_type == 3:\n",
    "          self.sim_reward.update_keys_held(action, True)\n",
    "          \n",
    "      time.sleep(0.04)\n",
    "      \n",
    "      if action_type in [1, 2]:\n",
    "        self.keyboard.release(self.keys_reference[action])\n",
    "        if action_type == 1:\n",
    "          self.sim_reward.update_keys_held(action, False)\n",
    "  \n",
    "  def _vision_setup(self):\n",
    "    if os.name == 'nt':\n",
    "      pathlib.PosixPath = pathlib.WindowsPath # https://github.com/ultralytics/yolov5/issues/10240#issuecomment-1662573188\n",
    "    \n",
    "    if self.render_mode:\n",
    "      monitor = mss.mss().monitors[-1]\n",
    "      t, l, w, h = monitor['top'], monitor['left'], monitor['width'], monitor['height']\n",
    "      self.capture_region = {'left': l+int(w * 0.338), 'top': t, 'width': w-int(w * 0.673), 'height': h} \n",
    "      \n",
    "    self.model = torch.hub.load('ultralytics/yolov5', 'custom', path='./models/best.pt', force_reload=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\tiany/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-11-29 Python-3.10.6 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AC_Net(\n",
      "  (fc1): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      "  (actors): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      "  (critic): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "osu_env = SimOsuEnvironment(max_notes=4)\n",
    "ac_net = AC_Net(osu_env.observation_space.shape, osu_env.action_space)\n",
    "print(ac_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiDiscrete([4 4 4 4])\n",
      "Box(0.0, inf, (4, 3), float32)\n",
      "[[          0           0           0]\n",
      " [          0           0           0]\n",
      " [          0           0           0]\n",
      " [          0           0           0]]\n"
     ]
    }
   ],
   "source": [
    "print(osu_env.action_space)\n",
    "print(osu_env.observation_space)\n",
    "print(osu_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "  ac_net,\n",
    "  osu_env,\n",
    "  optimizer,\n",
    "  max_episode,\n",
    "  gamma=0.99,\n",
    "  beta=0.01,\n",
    "  grad_clip = 1.0\n",
    "):\n",
    "  total_rewards = []\n",
    "\n",
    "  for episode in range(max_episode):\n",
    "    terminated = False\n",
    "    states, multi_actions, rewards = [], [], []\n",
    "    hx = None\n",
    "\n",
    "    state = torch.tensor(osu_env.reset(), dtype=torch.float32)\n",
    "    while not terminated:\n",
    "      probs, value, hx = ac_net(state, hx)\n",
    "      action = []\n",
    "\n",
    "      for prob in probs:\n",
    "        prob = F.softmax(prob, dim=-1).squeeze()\n",
    "        action.append(torch.distributions.Categorical(prob).sample().item())\n",
    "\n",
    "      next_state, reward, terminated, _ = osu_env.step(action)\n",
    "\n",
    "      states.append(state)\n",
    "      rewards.append(reward)\n",
    "      multi_actions.append(action)\n",
    "\n",
    "      state = torch.tensor(next_state, dtype=torch.float32)\n",
    "    R = 0 if terminated else value\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "      R = torch.tensor(rewards[i], dtype=torch.float32) + gamma * R\n",
    "      R = R.detach()\n",
    "\n",
    "      probs, value, hx = ac_net(states[i], hx)\n",
    "      \n",
    "      advantage = R - value\n",
    "      policy_loss = []\n",
    "      for i_a, prob in enumerate(probs):\n",
    "        actions = multi_actions[i]\n",
    "        softmax_probs = F.softmax(prob, dim=-1).squeeze()\n",
    "\n",
    "        categorical_dist = torch.distributions.Categorical(softmax_probs)\n",
    "        entropy = categorical_dist.entropy().sum() * beta\n",
    "\n",
    "        log_probs = torch.log(softmax_probs)\n",
    "        policy_loss.append(-log_probs[actions[i_a]] * advantage + entropy)\n",
    "\n",
    "    value_loss = F.mse_loss(value, R)\n",
    "\n",
    "    loss = torch.sum(torch.stack(policy_loss)) + value_loss \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(ac_net.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    total_rewards.append(sum(rewards))\n",
    "    meta_data = osu_env.get_meta_data()\n",
    "    print(f'Episode {episode:<5} {\"[\" + meta_data[\"song_name\"] + \"] \" + str(meta_data[\"difficulty\"]):<50} Reward: {sum(rewards):>10.4f}, loss: {loss.item():>10.4f}')\n",
    "    print(osu_env.sim_reward.get_debug())\n",
    "    print(osu_env.total_invalid_actions)\n",
    "    print(osu_env.actions_taken)\n",
    "    \n",
    "  return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "  ac_net,\n",
    "  osu_env,\n",
    "  max_episode\n",
    "):\n",
    "  total_rewards = []\n",
    "  hx = None\n",
    "\n",
    "  for episode in range(max_episode):\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    states, multi_actions, rewards = [], [], []\n",
    "\n",
    "    state = torch.tensor(osu_env.reset(), dtype=torch.float32)\n",
    "    while not terminated:\n",
    "      probs, value, hx = ac_net(state)\n",
    "      action = []\n",
    "\n",
    "      for prob in probs:\n",
    "        prob = F.softmax(prob, dim=-1).squeeze()\n",
    "        action.append(torch.argmax(prob).item())\n",
    "\n",
    "      next_state, reward, terminated, _ = osu_env.step(action)\n",
    "\n",
    "      states.append(state)\n",
    "      rewards.append(reward)\n",
    "      multi_actions.append(action)\n",
    "\n",
    "      state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "    total_rewards.append(sum(rewards))\n",
    "    meta_data = osu_env.get_meta_data()\n",
    "    print(f'Episode {episode:<5} {\"[\" + meta_data[\"song_name\"] + \"] \" + str(meta_data[\"difficulty\"]):<50} Reward: {sum(rewards):>10.4f}')\n",
    "    print(osu_env.sim_reward.get_debug())\n",
    "    print(osu_env.total_invalid_actions)\n",
    "    print(osu_env.actions_taken)\n",
    "    \n",
    "  return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0     [Cirno Break] 2.11                                 Reward: -1120.0590, loss:  4800.7808\n",
      "({'good_regular_notes': 460, 'good_end_holds': 0, 'good_hold': 53}, {'bad_hold': 68, 'broken_hold': 1693, 'bad_press': 699, 'bad_release': 17, 'missed_notes': 173, 'unnecessary_press': 0})\n",
      "1597\n",
      "[[        358         255         640         272]\n",
      " [        257         440         410         418]\n",
      " [        332         375         487         331]\n",
      " [        409         444         324         348]]\n",
      "Episode 1     [Ahoy!! Warera Houshou Kaizokudan] 2.12            Reward:  -807.1525, loss:  2990.8501\n",
      "({'good_regular_notes': 456, 'good_end_holds': 0, 'good_hold': 155}, {'bad_hold': 140, 'broken_hold': 1242, 'bad_press': 711, 'bad_release': 45, 'missed_notes': 209, 'unnecessary_press': 0})\n",
      "1406\n",
      "[[        365         265         573         174]\n",
      " [        297         387         417         276]\n",
      " [        260         340         437         340]\n",
      " [        409         380         301         287]]\n",
      "Episode 2     [Venom (Cut Ver.)] 2.78                            Reward:  -860.8858, loss:   516.4855\n",
      "({'good_regular_notes': 795, 'good_end_holds': 0, 'good_hold': 597}, {'bad_hold': 675, 'broken_hold': 1375, 'bad_press': 1178, 'bad_release': 170, 'missed_notes': 341, 'unnecessary_press': 0})\n",
      "1995\n",
      "[[        552         376         734         210]\n",
      " [        371         587         623         291]\n",
      " [        330         468         690         384]\n",
      " [        631         522         360         359]]\n",
      "Episode 3     [triangles] 1.19                                   Reward: -2291.7582, loss: 30494.6660\n",
      "({'good_regular_notes': 158, 'good_end_holds': 0, 'good_hold': 210}, {'bad_hold': 204, 'broken_hold': 1719, 'bad_press': 221, 'bad_release': 66, 'missed_notes': 259, 'unnecessary_press': 0})\n",
      "1671\n",
      "[[        426         352         532         304]\n",
      " [        346         481         297         490]\n",
      " [        227         437         442         508]\n",
      " [        534         505         220         355]]\n",
      "Episode 4     [Where Our Blue Is (TV size)] 2.02                 Reward: -1423.5392, loss:  3271.9927\n",
      "({'good_regular_notes': 304, 'good_end_holds': 0, 'good_hold': 169}, {'bad_hold': 187, 'broken_hold': 1181, 'bad_press': 491, 'bad_release': 51, 'missed_notes': 143, 'unnecessary_press': 0})\n",
      "1233\n",
      "[[        353         223         429         140]\n",
      " [        231         410         271         233]\n",
      " [        150         276         388         331]\n",
      " [        434         345         133         233]]\n",
      "Episode 5     [INTERNET OVERDOSE] 2.11                           Reward: -1406.5868, loss:  1010.8258\n",
      "({'good_regular_notes': 829, 'good_end_holds': 0, 'good_hold': 551}, {'bad_hold': 575, 'broken_hold': 2842, 'bad_press': 1368, 'bad_release': 158, 'missed_notes': 464, 'unnecessary_press': 0})\n",
      "3147\n",
      "[[        813         651        1056         352]\n",
      " [        550        1024         731         567]\n",
      " [        302         726         968         876]\n",
      " [        986         850         331         705]]\n",
      "Episode 6     [Where Our Blue Is (TV size)] 2.02                 Reward: -1375.1252, loss: 16206.5059\n",
      "({'good_regular_notes': 320, 'good_end_holds': 0, 'good_hold': 150}, {'bad_hold': 176, 'broken_hold': 1247, 'bad_press': 519, 'bad_release': 50, 'missed_notes': 145, 'unnecessary_press': 0})\n",
      "1238\n",
      "[[        302         255         453         135]\n",
      " [        228         404         285         228]\n",
      " [        140         247         399         359]\n",
      " [        408         366         103         268]]\n",
      "Episode 7     [Venom (Cut Ver.)] 2.78                            Reward: -1141.3600, loss:   953.8751\n",
      "({'good_regular_notes': 786, 'good_end_holds': 0, 'good_hold': 607}, {'bad_hold': 637, 'broken_hold': 1788, 'bad_press': 1132, 'bad_release': 169, 'missed_notes': 387, 'unnecessary_press': 0})\n",
      "1990\n",
      "[[        519         409         728         216]\n",
      " [        395         703         447         327]\n",
      " [        198         418         719         537]\n",
      " [        686         488         188         510]]\n",
      "Episode 8     [Six Trillion Years and Overnight Story] 2.93      Reward: -1693.5561, loss: 11631.3086\n",
      "({'good_regular_notes': 539, 'good_end_holds': 0, 'good_hold': 27}, {'bad_hold': 22, 'broken_hold': 1332, 'bad_press': 798, 'bad_release': 10, 'missed_notes': 269, 'unnecessary_press': 0})\n",
      "1206\n",
      "[[        309         265         436         124]\n",
      " [        265         422         253         194]\n",
      " [         94         227         433         380]\n",
      " [        429         275          95         335]]\n",
      "Episode 9     [Ahoy!! Warera Houshou Kaizokudan] 2.12            Reward: -1572.0648, loss:  7245.4263\n",
      "({'good_regular_notes': 454, 'good_end_holds': 0, 'good_hold': 148}, {'bad_hold': 139, 'broken_hold': 1538, 'bad_press': 650, 'bad_release': 36, 'missed_notes': 248, 'unnecessary_press': 0})\n",
      "1402\n",
      "[[        383         339         494         161]\n",
      " [        265         513         313         286]\n",
      " [        115         297         471         494]\n",
      " [        501         364         129         383]]\n",
      "Episode 10    [Venom (Cut Ver.)] 2.78                            Reward:  -866.9370, loss:   373.7040\n",
      "({'good_regular_notes': 778, 'good_end_holds': 0, 'good_hold': 700}, {'bad_hold': 623, 'broken_hold': 1878, 'bad_press': 1181, 'bad_release': 152, 'missed_notes': 347, 'unnecessary_press': 0})\n",
      "1941\n",
      "[[        480         520         691         181]\n",
      " [        423         697         456         296]\n",
      " [        156         359         783         574]\n",
      " [        644         380         183         665]]\n",
      "Episode 11    [Cirno Break] 2.11                                 Reward: -1302.9499, loss: 10719.6611\n",
      "({'good_regular_notes': 427, 'good_end_holds': 0, 'good_hold': 67}, {'bad_hold': 58, 'broken_hold': 2051, 'bad_press': 662, 'bad_release': 19, 'missed_notes': 177, 'unnecessary_press': 0})\n",
      "1506\n",
      "[[        476         354         503         192]\n",
      " [        269         574         292         390]\n",
      " [        101         306         541         577]\n",
      " [        435         428         102         560]]\n",
      "Episode 12    [Ahoy!! Warera Houshou Kaizokudan] 2.12            Reward: -1817.8138, loss: 22476.8164\n",
      "({'good_regular_notes': 423, 'good_end_holds': 0, 'good_hold': 145}, {'bad_hold': 143, 'broken_hold': 1895, 'bad_press': 643, 'bad_release': 43, 'missed_notes': 254, 'unnecessary_press': 0})\n",
      "1326\n",
      "[[        430         362         402         183]\n",
      " [        248         501         233         395]\n",
      " [        106         272         513         486]\n",
      " [        410         329          90         548]]\n",
      "Episode 13    [Sway to My Beat in Cosmos] 1.5                    Reward: -1721.5040, loss:  3445.5056\n",
      "({'good_regular_notes': 407, 'good_end_holds': 0, 'good_hold': 300}, {'bad_hold': 289, 'broken_hold': 2575, 'bad_press': 574, 'bad_release': 85, 'missed_notes': 225, 'unnecessary_press': 0})\n",
      "2068\n",
      "[[        529         556         638         243]\n",
      " [        307         847         313         499]\n",
      " [        103         360         736         767]\n",
      " [        519         492         137         818]]\n",
      "Episode 14    [Ahoy!! Warera Houshou Kaizokudan] 2.12            Reward: -1595.1417, loss: 12833.6924\n",
      "({'good_regular_notes': 437, 'good_end_holds': 0, 'good_hold': 190}, {'bad_hold': 145, 'broken_hold': 1755, 'bad_press': 671, 'bad_release': 45, 'missed_notes': 240, 'unnecessary_press': 0})\n",
      "1430\n",
      "[[        416         401         406         154]\n",
      " [        224         578         209         366]\n",
      " [         73         263         556         485]\n",
      " [        330         345          82         620]]\n",
      "Episode 15    [INTERNET OVERDOSE] 2.11                           Reward: -1244.5509, loss:  1599.1678\n",
      "({'good_regular_notes': 837, 'good_end_holds': 0, 'good_hold': 675}, {'bad_hold': 568, 'broken_hold': 3601, 'bad_press': 1358, 'bad_release': 158, 'missed_notes': 449, 'unnecessary_press': 0})\n",
      "3096\n",
      "[[        751         912         896         313]\n",
      " [        367        1242         511         752]\n",
      " [        102         487        1316         967]\n",
      " [        645         666         158        1403]]\n",
      "Episode 16    [Shelter] 1.41                                     Reward: -2306.2937, loss:  3904.1904\n",
      "({'good_regular_notes': 252, 'good_end_holds': 0, 'good_hold': 145}, {'bad_hold': 124, 'broken_hold': 1961, 'bad_press': 389, 'bad_release': 30, 'missed_notes': 195, 'unnecessary_press': 0})\n",
      "1684\n",
      "[[        344         556         464         162]\n",
      " [        217         767         187         355]\n",
      " [         41         208         712         565]\n",
      " [        278         371          56         821]]\n",
      "Episode 17    [Cirno Break] 2.11                                 Reward: -1159.8820, loss:  7610.0352\n",
      "({'good_regular_notes': 441, 'good_end_holds': 0, 'good_hold': 71}, {'bad_hold': 62, 'broken_hold': 2082, 'bad_press': 718, 'bad_release': 16, 'missed_notes': 169, 'unnecessary_press': 0})\n",
      "1607\n",
      "[[        368         503         474         180]\n",
      " [        194         744         215         372]\n",
      " [         27         211         822         465]\n",
      " [        283         356          40         846]]\n",
      "Episode 18    [INTERNET OVERDOSE] 2.11                           Reward: -1352.8743, loss:  1837.2075\n",
      "({'good_regular_notes': 870, 'good_end_holds': 0, 'good_hold': 710}, {'bad_hold': 574, 'broken_hold': 3526, 'bad_press': 1370, 'bad_release': 163, 'missed_notes': 465, 'unnecessary_press': 0})\n",
      "3098\n",
      "[[        722         947         897         306]\n",
      " [        360        1346         397         769]\n",
      " [         57         374        1587         854]\n",
      " [        424         697          88        1663]]\n",
      "Episode 19    [Brain Power] 1.36                                 Reward: -2799.4609, loss: 35655.0664\n",
      "({'good_regular_notes': 225, 'good_end_holds': 0, 'good_hold': 112}, {'bad_hold': 78, 'broken_hold': 1867, 'bad_press': 333, 'bad_release': 25, 'missed_notes': 200, 'unnecessary_press': 0})\n",
      "1786\n",
      "[[        302         610         460         114]\n",
      " [        137         873         145         331]\n",
      " [         30         164         775         517]\n",
      " [        203         318          40         925]]\n",
      "Episode 20    [INTERNET OVERDOSE] 2.11                           Reward: -1192.1557, loss:   969.5170\n",
      "({'good_regular_notes': 870, 'good_end_holds': 0, 'good_hold': 721}, {'bad_hold': 571, 'broken_hold': 3408, 'bad_press': 1406, 'bad_release': 172, 'missed_notes': 450, 'unnecessary_press': 0})\n",
      "3141\n",
      "[[        669        1037         894         272]\n",
      " [        297        1511         342         722]\n",
      " [         45         303        1792         732]\n",
      " [        337         634          74        1827]]\n",
      "Episode 21    [Shelter] 1.41                                     Reward: -2227.9720, loss: 10876.8965\n",
      "({'good_regular_notes': 255, 'good_end_holds': 0, 'good_hold': 144}, {'bad_hold': 107, 'broken_hold': 1891, 'bad_press': 381, 'bad_release': 35, 'missed_notes': 199, 'unnecessary_press': 0})\n",
      "1726\n",
      "[[        309         573         503         141]\n",
      " [        143         882         143         358]\n",
      " [         16         107         980         423]\n",
      " [        160         327          25        1014]]\n",
      "Episode 22    [Brain Power] 1.36                                 Reward: -2816.4420, loss: 28783.2148\n",
      "({'good_regular_notes': 219, 'good_end_holds': 0, 'good_hold': 78}, {'bad_hold': 83, 'broken_hold': 1745, 'bad_press': 328, 'bad_release': 30, 'missed_notes': 201, 'unnecessary_press': 0})\n",
      "1801\n",
      "[[        273         578         519         116]\n",
      " [        141         966         103         276]\n",
      " [         11          98         998         379]\n",
      " [        137         309          30        1010]]\n",
      "Episode 23    [Where Our Blue Is (TV size)] 2.02                 Reward: -1664.9416, loss:  7405.1553\n",
      "({'good_regular_notes': 350, 'good_end_holds': 0, 'good_hold': 216}, {'bad_hold': 197, 'broken_hold': 1360, 'bad_press': 477, 'bad_release': 60, 'missed_notes': 167, 'unnecessary_press': 0})\n",
      "1252\n",
      "[[        278         377         391          99]\n",
      " [        105         712          93         235]\n",
      " [          5          74         842         224]\n",
      " [         85         231          13         816]]\n",
      "Episode 24    [Where Our Blue Is (TV size)] 2.02                 Reward: -1170.2838, loss:  9729.4092\n",
      "({'good_regular_notes': 357, 'good_end_holds': 0, 'good_hold': 229}, {'bad_hold': 200, 'broken_hold': 1355, 'bad_press': 501, 'bad_release': 50, 'missed_notes': 148, 'unnecessary_press': 0})\n",
      "1178\n",
      "[[        262         387         419          77]\n",
      " [        105         681          99         260]\n",
      " [         12          62         869         202]\n",
      " [         64         217          10         854]]\n",
      "Episode 25    [triangles] 1.19                                   Reward: -2033.5165, loss: 20186.8203\n",
      "({'good_regular_notes': 159, 'good_end_holds': 0, 'good_hold': 247}, {'bad_hold': 193, 'broken_hold': 1575, 'bad_press': 248, 'bad_release': 75, 'missed_notes': 264, 'unnecessary_press': 0})\n",
      "1965\n",
      "[[        243         576         704          91]\n",
      " [        127        1158          73         256]\n",
      " [          3          59        1205         347]\n",
      " [        108         326          13        1167]]\n",
      "Episode 26    [Six Trillion Years and Overnight Story] 2.93      Reward: -1420.2864, loss:  7467.5303\n",
      "({'good_regular_notes': 568, 'good_end_holds': 0, 'good_hold': 35}, {'bad_hold': 29, 'broken_hold': 1321, 'bad_press': 854, 'bad_release': 10, 'missed_notes': 250, 'unnecessary_press': 0})\n",
      "1316\n",
      "[[        235         395         432          72]\n",
      " [         70         792          76         196]\n",
      " [          9          51         886         188]\n",
      " [         60         226          11         837]]\n",
      "Episode 27    [Ahoy!! Warera Houshou Kaizokudan] 2.12            Reward: -1532.1188, loss: 15824.0254\n",
      "({'good_regular_notes': 436, 'good_end_holds': 0, 'good_hold': 215}, {'bad_hold': 155, 'broken_hold': 1336, 'bad_press': 691, 'bad_release': 47, 'missed_notes': 236, 'unnecessary_press': 0})\n",
      "1663\n",
      "[[        249         436         636          56]\n",
      " [         60        1069          55         193]\n",
      " [          5          48        1108         216]\n",
      " [         40         292          11        1034]]\n",
      "Episode 28    [Brain Power] 1.36                                 Reward: -2639.3531, loss: 17057.0820\n",
      "({'good_regular_notes': 214, 'good_end_holds': 0, 'good_hold': 95}, {'bad_hold': 82, 'broken_hold': 1483, 'bad_press': 365, 'bad_release': 28, 'missed_notes': 193, 'unnecessary_press': 0})\n",
      "1841\n",
      "[[        194         481         757          54]\n",
      " [         67        1214          34         171]\n",
      " [          1          47        1209         229]\n",
      " [         47         278           5        1156]]\n",
      "Episode 29    [Where Our Blue Is (TV size)] 2.02                 Reward: -1233.3890, loss: 11253.7412\n",
      "({'good_regular_notes': 335, 'good_end_holds': 0, 'good_hold': 185}, {'bad_hold': 200, 'broken_hold': 1152, 'bad_press': 519, 'bad_release': 55, 'missed_notes': 142, 'unnecessary_press': 0})\n",
      "1403\n",
      "[[        139         355         598          53]\n",
      " [         54         933          42         116]\n",
      " [          2          35         959         149]\n",
      " [         35         221           4         885]]\n",
      "Episode 30    [Brain Power] 1.36                                 Reward: -2803.7736, loss: 27257.2812\n",
      "({'good_regular_notes': 221, 'good_end_holds': 0, 'good_hold': 90}, {'bad_hold': 75, 'broken_hold': 1401, 'bad_press': 339, 'bad_release': 30, 'missed_notes': 202, 'unnecessary_press': 0})\n",
      "1940\n",
      "[[        206         426         800          54]\n",
      " [         45        1317          15         109]\n",
      " [          1          28        1239         218]\n",
      " [         31         286           3        1166]]\n",
      "Episode 31    [Where Our Blue Is (TV size)] 2.02                 Reward: -1390.6511, loss: 10110.0166\n",
      "({'good_regular_notes': 339, 'good_end_holds': 0, 'good_hold': 181}, {'bad_hold': 192, 'broken_hold': 1039, 'bad_press': 501, 'bad_release': 59, 'missed_notes': 148, 'unnecessary_press': 0})\n",
      "1447\n",
      "[[        148         336         625          36]\n",
      " [         30         992          25          98]\n",
      " [          1          31         993         120]\n",
      " [         22         257           3         863]]\n",
      "Episode 32    [Ahoy!! Warera Houshou Kaizokudan] 2.12            Reward: -1342.5101, loss:  7724.6396\n",
      "({'good_regular_notes': 460, 'good_end_holds': 0, 'good_hold': 143}, {'bad_hold': 157, 'broken_hold': 1207, 'bad_press': 717, 'bad_release': 49, 'missed_notes': 229, 'unnecessary_press': 0})\n",
      "1708\n",
      "[[        156         313         874          34]\n",
      " [         39        1223          26          89]\n",
      " [          0          21        1201         155]\n",
      " [         18         299           4        1056]]\n",
      "Episode 33    [Cirno Break] 2.11                                 Reward: -1159.2920, loss:  2205.8284\n",
      "({'good_regular_notes': 474, 'good_end_holds': 0, 'good_hold': 47}, {'bad_hold': 80, 'broken_hold': 1421, 'bad_press': 697, 'bad_release': 18, 'missed_notes': 172, 'unnecessary_press': 0})\n",
      "1905\n",
      "[[        149         317        1009          50]\n",
      " [         24        1394          14          93]\n",
      " [          2          30        1337         156]\n",
      " [         22         337           3        1163]]\n",
      "Episode 34    [Where Our Blue Is (TV size)] 2.02                 Reward: -1459.0985, loss:  4655.5879\n",
      "({'good_regular_notes': 345, 'good_end_holds': 0, 'good_hold': 213}, {'bad_hold': 201, 'broken_hold': 954, 'bad_press': 522, 'bad_release': 57, 'missed_notes': 150, 'unnecessary_press': 0})\n",
      "1395\n",
      "[[        105         218         797          25]\n",
      " [         16        1058          16          55]\n",
      " [          0          17        1032          96]\n",
      " [          9         222           3         911]]\n",
      "Episode 35    [Venom (Cut Ver.)] 2.78                            Reward:  -657.3300, loss:   289.7855\n",
      "({'good_regular_notes': 858, 'good_end_holds': 0, 'good_hold': 724}, {'bad_hold': 752, 'broken_hold': 1291, 'bad_press': 1277, 'bad_release': 198, 'missed_notes': 320, 'unnecessary_press': 0})\n",
      "2230\n",
      "[[        109         285        1434          44]\n",
      " [         27        1744          20          81]\n",
      " [          0          31        1718         123]\n",
      " [         23         419           5        1425]]\n",
      "Episode 36    [Sway to My Beat in Cosmos] 1.5                    Reward: -1464.1161, loss:  4557.6729\n",
      "({'good_regular_notes': 435, 'good_end_holds': 0, 'good_hold': 336}, {'bad_hold': 357, 'broken_hold': 1498, 'bad_press': 661, 'bad_release': 99, 'missed_notes': 211, 'unnecessary_press': 0})\n",
      "2292\n",
      "[[         98         262        1565          41]\n",
      " [         23        1882          10          51]\n",
      " [          1          20        1869          76]\n",
      " [         11         388           4        1563]]\n",
      "Episode 37    [Sway to My Beat in Cosmos] 1.5                    Reward: -1365.9631, loss:  4199.9316\n",
      "({'good_regular_notes': 436, 'good_end_holds': 0, 'good_hold': 367}, {'bad_hold': 364, 'broken_hold': 1480, 'bad_press': 667, 'bad_release': 101, 'missed_notes': 208, 'unnecessary_press': 0})\n",
      "2293\n",
      "[[         63         221        1640          42]\n",
      " [         25        1896           9          36]\n",
      " [          1          19        1851          95]\n",
      " [          8         378           2        1578]]\n",
      "Episode 38    [Cirno Break] 2.11                                 Reward: -1088.9381, loss:  3613.4751\n",
      "({'good_regular_notes': 470, 'good_end_holds': 0, 'good_hold': 50}, {'bad_hold': 82, 'broken_hold': 1263, 'bad_press': 731, 'bad_release': 21, 'missed_notes': 175, 'unnecessary_press': 0})\n",
      "1756\n",
      "[[         54         145        1300          26]\n",
      " [          7        1486           6          26]\n",
      " [          0          12        1449          64]\n",
      " [          3         324           1        1197]]\n",
      "Episode 39    [Where Our Blue Is (TV size)] 2.02                 Reward: -1117.5292, loss:  5460.8296\n",
      "({'good_regular_notes': 374, 'good_end_holds': 0, 'good_hold': 210}, {'bad_hold': 210, 'broken_hold': 823, 'bad_press': 575, 'bad_release': 58, 'missed_notes': 144, 'unnecessary_press': 0})\n",
      "1295\n",
      "[[         28          81        1020          16]\n",
      " [          4        1121           6          14]\n",
      " [          1           7        1110          27]\n",
      " [          3         251           2         889]]\n",
      "Episode 40    [Cirno Break] 2.11                                 Reward: -1042.7729, loss:   166.7613\n",
      "({'good_regular_notes': 491, 'good_end_holds': 0, 'good_hold': 77}, {'bad_hold': 87, 'broken_hold': 1207, 'bad_press': 743, 'bad_release': 20, 'missed_notes': 179, 'unnecessary_press': 0})\n",
      "1691\n",
      "[[         33          93        1382          17]\n",
      " [         11        1492           7          15]\n",
      " [          0          17        1477          31]\n",
      " [          3         304           1        1217]]\n",
      "Episode 41    [Six Trillion Years and Overnight Story] 2.93      Reward:  -649.2840, loss:   305.1653\n",
      "({'good_regular_notes': 643, 'good_end_holds': 0, 'good_hold': 45}, {'bad_hold': 55, 'broken_hold': 836, 'bad_press': 991, 'bad_release': 12, 'missed_notes': 211, 'unnecessary_press': 0})\n",
      "1253\n",
      "[[         21          43        1058          12]\n",
      " [          6        1112           4          12]\n",
      " [          0           7        1105          22]\n",
      " [          1         293           0         840]]\n",
      "Episode 42    [Brain Power] 1.36                                 Reward: -1838.2749, loss:  6361.6577\n",
      "({'good_regular_notes': 246, 'good_end_holds': 0, 'good_hold': 68}, {'bad_hold': 90, 'broken_hold': 1166, 'bad_press': 372, 'bad_release': 27, 'missed_notes': 184, 'unnecessary_press': 0})\n",
      "1621\n",
      "[[         12          52        1415           7]\n",
      " [          4        1472           1           9]\n",
      " [          2           7        1445          32]\n",
      " [          3         260           0        1223]]\n",
      "Episode 43    [Six Trillion Years and Overnight Story] 2.93      Reward:  -644.8687, loss:   161.9971\n",
      "({'good_regular_notes': 655, 'good_end_holds': 0, 'good_hold': 49}, {'bad_hold': 52, 'broken_hold': 879, 'bad_press': 959, 'bad_release': 15, 'missed_notes': 214, 'unnecessary_press': 0})\n",
      "1233\n",
      "[[         16          38        1069          11]\n",
      " [          2        1121           3           8]\n",
      " [          0           5        1110          19]\n",
      " [          5         244           0         885]]\n",
      "Episode 44    [Venom (Cut Ver.)] 2.78                            Reward:  -319.3387, loss:   507.6320\n",
      "({'good_regular_notes': 898, 'good_end_holds': 0, 'good_hold': 714}, {'bad_hold': 819, 'broken_hold': 1093, 'bad_press': 1399, 'bad_release': 206, 'missed_notes': 284, 'unnecessary_press': 0})\n",
      "2057\n",
      "[[         16          59        1785          12]\n",
      " [          6        1847           4          15]\n",
      " [          1           7        1833          31]\n",
      " [          6         462           1        1403]]\n",
      "Episode 45    [Where Our Blue Is (TV size)] 2.02                 Reward:  -960.6010, loss:   980.6411\n",
      "({'good_regular_notes': 380, 'good_end_holds': 0, 'good_hold': 213}, {'bad_hold': 221, 'broken_hold': 791, 'bad_press': 583, 'bad_release': 58, 'missed_notes': 138, 'unnecessary_press': 0})\n",
      "1226\n",
      "[[          7          19        1112           7]\n",
      " [          3        1133           4           5]\n",
      " [          0           4        1127          14]\n",
      " [          1         245           1         898]]\n",
      "Episode 46    [Venom (Cut Ver.)] 2.78                            Reward:  -361.0730, loss:   237.0551\n",
      "({'good_regular_notes': 938, 'good_end_holds': 0, 'good_hold': 680}, {'bad_hold': 824, 'broken_hold': 1099, 'bad_press': 1380, 'bad_release': 199, 'missed_notes': 289, 'unnecessary_press': 0})\n",
      "2035\n",
      "[[         10          28        1825           9]\n",
      " [          1        1863           2           6]\n",
      " [          0           3        1844          25]\n",
      " [          4         449           0        1419]]\n",
      "Episode 47    [Shelter] 1.41                                     Reward: -1453.1469, loss:  2648.1997\n",
      "({'good_regular_notes': 285, 'good_end_holds': 0, 'good_hold': 110}, {'bad_hold': 138, 'broken_hold': 1157, 'bad_press': 450, 'bad_release': 33, 'missed_notes': 182, 'unnecessary_press': 0})\n",
      "1617\n",
      "[[          5          17        1495           9]\n",
      " [          0        1516           0          10]\n",
      " [          0           4        1499          23]\n",
      " [          0         272           2        1252]]\n",
      "Episode 48    [Where Our Blue Is (TV size)] 2.02                 Reward:  -932.0534, loss:  3404.9556\n",
      "({'good_regular_notes': 381, 'good_end_holds': 0, 'good_hold': 150}, {'bad_hold': 212, 'broken_hold': 887, 'bad_press': 576, 'bad_release': 57, 'missed_notes': 138, 'unnecessary_press': 0})\n",
      "1225\n",
      "[[          1          10        1126           8]\n",
      " [          3        1140           0           2]\n",
      " [          1           2        1121          21]\n",
      " [          1         224           1         919]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[470], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m max_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(ac_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mac_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mosu_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episode\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[468], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(ac_net, osu_env, optimizer, max_episode, gamma, beta, grad_clip)\u001b[0m\n\u001b[0;32m     23\u001b[0m   prob \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(prob, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     24\u001b[0m   action\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(prob)\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 26\u001b[0m next_state, reward, terminated, _ \u001b[38;5;241m=\u001b[39m \u001b[43mosu_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[0;32m     29\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Cell \u001b[1;32mIn[465], line 82\u001b[0m, in \u001b[0;36mSimOsuEnvironment.step\u001b[1;34m(self, multi_actions)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img:\n\u001b[0;32m     81\u001b[0m   vision_thread \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39msubmit(detect, img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m---> 82\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation \u001b[38;5;241m=\u001b[39m \u001b[43mvision_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_notes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation)\n\u001b[0;32m     85\u001b[0m actions, action_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_multi_actions(multi_actions)\n",
      "File \u001b[1;32mc:\\Users\\tiany\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\tiany\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_episode = 100\n",
    "optimizer = torch.optim.Adam(ac_net.parameters(), lr=0.001)\n",
    "total_rewards = train(ac_net, osu_env, optimizer, max_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rewards = test(ac_net, osu_env, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ac_net.state_dict(), 'models/ac_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(total_rewards)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
