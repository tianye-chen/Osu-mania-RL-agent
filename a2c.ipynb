{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from reward import Reward\n",
    "from helper import detect, capture, pad_inner_array, SocketListener, DataQueue\n",
    "from pynput import keyboard\n",
    "from pynput.keyboard import Controller, Key\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import deque\n",
    "from a2c_agent import A2C_Agent\n",
    "import mss\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('yolov5'):\n",
    "    !git clone https://github.com/ultralytics/yolov5\n",
    "    !pip install -r yolov5/requirements.txt\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "logging.getLogger('ultralytics').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC_Net(nn.Module):\n",
    "  def __init__(self, input, action_space):\n",
    "    super(AC_Net, self).__init__()\n",
    "    self.lstm = nn.LSTM(input[1] * input[2], 128, batch_first=True)\n",
    "    self.fc1 = nn.Linear(128, 128)\n",
    "    self.fc2 = nn.Linear(128, 128)\n",
    "    self.actors = nn.ModuleList([nn.Linear(128, action.n) for action in action_space])\n",
    "    self.critic = nn.Linear(128, 1)\n",
    "    \n",
    "    for layer in [self.fc1, self.fc2, *self.actors, self.critic]:\n",
    "      nn.init.xavier_uniform_(layer.weight)\n",
    "      nn.init.constant_(layer.bias, 0)\n",
    "    \n",
    "  def forward(self, x, hx=None):\n",
    "    x = x.view(1, x.size(0), -1)\n",
    "    x, hx = self.lstm(x, hx)\n",
    "    x = x[:, -1, :]\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = nn.Dropout(0.3)(x)\n",
    "     \n",
    "    return [actor(x) for actor in self.actors], self.critic(x), hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: DELETE LATER\n",
    "def printd(message, debug_file=\"C:\\\\tmp\\\\debug_output.txt\"):\n",
    "  with open(debug_file, \"a\") as debug_console:\n",
    "      debug_console.write(str(message) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimOsuEnvironment(gym.Env):\n",
    "  def __init__(self, stacked_frames=1, max_notes=8, render_mode=True):\n",
    "    # Common parameters\n",
    "    self.max_notes = max_notes\n",
    "    self.stacked_frames = stacked_frames\n",
    "    self.keys_reference = ['s', 'd', 'k', 'l'] # Used for keyboard input with index corresponding to a lane\n",
    "    self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(self.stacked_frames, self.max_notes, 3))\n",
    "    self.action_space = gym.spaces.MultiDiscrete([4] * len(self.keys_reference))\n",
    "    self.observation = np.zeros((self.max_notes, 3))\n",
    "    self.frame_buf = deque(maxlen=self.stacked_frames)\n",
    "    self.terminated = False\n",
    "  \n",
    "    self.reward_scale = 1 # Scaling reward to avoid extremely small values\n",
    "    self.invalid_actions = 0 # Counts the number of invalid actions in a step\n",
    "    self.invalid_action_penalty = -0.1\n",
    "    self.invalid_action_penalty_scaled = 0 # Scaled invalid action penalty, to be set in reset\n",
    "    self.total_invalid_actions = 0\n",
    "    self.actions_taken = np.zeros((4, 4))\n",
    "    \n",
    "    self.executor = ThreadPoolExecutor()\n",
    "    self.reward_func = Reward()\n",
    "    self.model = None \n",
    "    \n",
    "    self.frame_notes = [] # A note representation of each frame\n",
    "    \n",
    "    # Parameters for render mode\n",
    "    if render_mode:\n",
    "      self.render_mode = render_mode\n",
    "      self.capture_region = None\n",
    "      self.keyboard = Controller()\n",
    "      self.data_queue = DataQueue()\n",
    "      self.listener = None\n",
    "      self.listener = SocketListener()\n",
    "      self.listener.start(data_handler=self.data_queue.add, traceback=False)\n",
    "      \n",
    "    self._vision_setup()\n",
    "    \n",
    "    # Parameters for simulated mode\n",
    "    self.custom_reward = [8, 10, 1, -3, -2, -1, -1, -2, -1] # Custom reward function to be used with Reward class\n",
    "    self.custom_reward_scaled = []  # Scaled custom reward, to be set in reset\n",
    "    self.chosen_folder = None # A randomly chosen folder from available_folders, to be set in reset\n",
    "    self.available_folders = os.listdir('./frames/rl_simulated_training_sample')\n",
    "    self.frames = [] # A list of frames in the chosen folder, to be set in reset\n",
    "    self.metadata = { # Metadata of the chosen song, to be set in reset\n",
    "      'song_name': \"No metadata in render mode\",\n",
    "      'song_duration': 0,\n",
    "      'note_count': 0,\n",
    "      'hold_note_count': 0,\n",
    "      'difficulty': 0\n",
    "    } \n",
    "    self.frame_step = 0\n",
    "    self.note_count = 0\n",
    "    \n",
    "  def reset(self):\n",
    "    for _ in range(self.stacked_frames):\n",
    "      self.frame_buf.append(np.zeros((self.max_notes, 3), dtype=np.int32))\n",
    "    self.observation = np.stack(self.frame_buf, axis=0)\n",
    "    self.invalid_actions = 0\n",
    "    self.total_invalid_actions = 0\n",
    "    self.actions_taken = np.zeros((4, 4))\n",
    "    self.terminated = False\n",
    "    \n",
    "    if self.render_mode:\n",
    "      if self.listener.has_connection:\n",
    "        print('Closing connection')\n",
    "        self.listener.close_connection()\n",
    "      \n",
    "      self.data_queue.clear()\n",
    "      self._get_random_song()\n",
    "    else:\n",
    "      random_folder = random.choice(self.available_folders)\n",
    "      self.chosen_folder = './frames/rl_simulated_training_sample/' + random_folder\n",
    "      self.frames = sorted(os.listdir(self.chosen_folder))[:-1]\n",
    "      self.frame_notes = []\n",
    "      self.frame_step = 0\n",
    "      \n",
    "      self.metadata = json.load(open(self.chosen_folder + '/metadata.json'))\n",
    "      self.note_count = self.metadata['note_count'] + self.metadata['hold_note_count'] * 2\n",
    "      \n",
    "      self.custom_reward_scaled = [(r / self.note_count) * self.reward_scale for r in self.custom_reward]\n",
    "      self.invalid_action_penalty_scaled = (self.invalid_action_penalty / self.note_count) * self.reward_scale\n",
    "      self.reward_func.set_custom_rewards(self.custom_reward_scaled)\n",
    "    \n",
    "    return self.observation\n",
    "  \n",
    "  def step(self, multi_actions):\n",
    "    if self.render_mode:\n",
    "      self.observation, reward = self._render_mode_step(multi_actions)\n",
    "    else:\n",
    "      self.observation, reward = self._simulated_mode_step(multi_actions)\n",
    "    \n",
    "    self.frame_buf.append(self.observation)\n",
    "    self.observation = np.stack(self.frame_buf, axis=0)\n",
    "    \n",
    "    self.total_invalid_actions += self.invalid_actions\n",
    "    self.invalid_actions = 0\n",
    "    info = {}\n",
    "    \n",
    "    return self.observation, reward, self.terminated, info\n",
    "    \n",
    "  def get_meta_data(self):\n",
    "    '''\n",
    "    Returns the metadata of the current song in the form of \n",
    "    {\n",
    "      'song_name' (str), \\\\\n",
    "      'song_duration' (int) in seconds, \\\\\n",
    "      'note_count' (int,) \\\\\n",
    "      'hold_note_count' (int), \\\\\n",
    "      'difficulty' (float)\n",
    "    }\n",
    "    '''\n",
    "    return self.metadata\n",
    "  \n",
    "  def _render_mode_step(self, multi_actions):\n",
    "    '''\n",
    "    Steps the environment in render mode, where the game is running and the game window is visible\n",
    "    '''\n",
    "    while self.listener.is_first_connection and not self.listener.has_connection:\n",
    "      pass\n",
    "    \n",
    "    img = capture(self.capture_region)\n",
    "    img = np.array(img)\n",
    "    \n",
    "    self._update_observation(img)\n",
    "    \n",
    "    actions, action_types = self._parse_multi_actions(multi_actions)\n",
    "    self.executor.submit(self._perform_keyboard_action, actions, action_types)\n",
    "    \n",
    "    data = self.data_queue.get()\n",
    "    self.data_queue.clear()\n",
    "    reward = self.reward_func.get_in_game_reward(data, self.frame_notes[-2] if len(self.frame_notes) > 3 else []) + self.invalid_action_penalty * self.invalid_actions\n",
    "    \n",
    "    if not self.terminated:\n",
    "      self.terminated = (not self.listener.is_first_connection and not self.listener.has_connection) or 6 in data or 7 in data\n",
    "    \n",
    "    self.executor.submit(printd, data)\n",
    "    \n",
    "    return self.observation, reward\n",
    "  \n",
    "  def _simulated_mode_step(self, multi_actions):\n",
    "    '''\n",
    "    Steps the environment in simulated mode, where the pre-recorded frames are used as input\n",
    "    '''\n",
    "    img = Image.open(self.chosen_folder + '/' + self.frames[self.frame_step])\n",
    "    self.frame_step += 1\n",
    "    \n",
    "    self._update_observation(img)\n",
    "    \n",
    "    actions, _ = self._parse_multi_actions(multi_actions)\n",
    "    reward = self.reward_func.get_simulated_reward(actions, self.frame_notes[-2] if len(self.frame_notes) > 3 else []) + self.invalid_action_penalty_scaled * self.invalid_actions\n",
    "    self.terminated = self.frame_step >= len(self.frames)\n",
    "    \n",
    "    return self.observation, reward\n",
    "  \n",
    "  def _update_observation(self, img):\n",
    "    vision_thread = self.executor.submit(detect, img, self.model)\n",
    "    self.observation = vision_thread.result()\n",
    "    self.frame_notes.append(self.observation)\n",
    "    self.observation = pad_inner_array([self.observation], [0, 0 ,0], self.max_notes)[0]\n",
    "    \n",
    "  def _parse_multi_actions(self, multi_actions):\n",
    "    '''\n",
    "    Parses the multi actions into a list of actions and their types\n",
    "    '''\n",
    "    parsed_actions = []\n",
    "    action_type = []\n",
    "    \n",
    "    for action_lane, action in enumerate(multi_actions):\n",
    "      self.actions_taken[action_lane][action] += 1\n",
    "      \n",
    "      match action:\n",
    "        case 0: # Do nothing\n",
    "          pass\n",
    "        case 1: # Release\n",
    "          if not self.reward_func.get_key_held(action_lane):\n",
    "            self.invalid_actions += 1\n",
    "            continue\n",
    "          \n",
    "          self.reward_func.update_keys_held(action_lane, False)\n",
    "          \n",
    "          parsed_actions.append(action_lane)\n",
    "        case 2: # Press\n",
    "          if self.reward_func.get_key_held(action_lane):\n",
    "            self.invalid_actions += 1\n",
    "            \n",
    "          self.reward_func.update_keys_held(action_lane, False)\n",
    "          parsed_actions.append(action_lane)\n",
    "        case 3: # Hold\n",
    "          if not self.reward_func.get_key_held(action_lane):\n",
    "            parsed_actions.append(action_lane)\n",
    "          \n",
    "            self.reward_func.update_keys_held(action_lane, True)\n",
    "\n",
    "      if action_type != 0:\n",
    "        action_type.append(action)\n",
    "          \n",
    "    return parsed_actions, action_type\n",
    "  \n",
    "  def _perform_keyboard_action(self, parsed_actions, action_type):\n",
    "    for action, action_type in zip(parsed_actions, action_type):\n",
    "      if action_type in [2, 3]:\n",
    "        self.keyboard.press(self.keys_reference[action])\n",
    "        if action_type == 3:\n",
    "          self.reward_func.update_keys_held(action, True)\n",
    "          \n",
    "      time.sleep(0.04)\n",
    "      \n",
    "      if action_type in [1, 2]:\n",
    "        self.keyboard.release(self.keys_reference[action])\n",
    "        if action_type == 1:\n",
    "          self.reward_func.update_keys_held(action, False)\n",
    "          \n",
    "  def _get_random_song(self):\n",
    "    '''\n",
    "    Choose a random song using in-game menu navigation, game must be in focus and in song selection screen\n",
    "    '''\n",
    "    key_sequence = ['a', Key.esc, Key.f2] + ([Key.down] * np.random.randint(0, 6)) + [Key.enter]\n",
    "    \n",
    "    time.sleep(2) \n",
    "    for key in key_sequence:\n",
    "      self.keyboard.press(key)\n",
    "      time.sleep(0.05)\n",
    "      self.keyboard.release(key)\n",
    "    \n",
    "    time.sleep(3)\n",
    "    self.executor.submit(self._skip_intro)\n",
    "  \n",
    "  def _skip_intro(self):\n",
    "    for _ in range(10):\n",
    "      self.keyboard.press(Key.space)\n",
    "      time.sleep(0.05)\n",
    "      self.keyboard.release(Key.space)\n",
    "      time.sleep(0.1)\n",
    "   \n",
    "  def _vision_setup(self):\n",
    "    '''\n",
    "    Sets up the vision model for object detection, also sets up capture region for render mode\n",
    "    '''\n",
    "    if os.name == 'nt':\n",
    "      pathlib.PosixPath = pathlib.WindowsPath # https://github.com/ultralytics/yolov5/issues/10240#issuecomment-1662573188\n",
    "    \n",
    "    if self.render_mode:\n",
    "      monitor = mss.mss().monitors[-1]\n",
    "      t, l, w, h = monitor['top'], monitor['left'], monitor['width'], monitor['height']\n",
    "      self.capture_region = {'left': l+int(w * 0.338), 'top': t, 'width': w-int(w * 0.673), 'height': h} \n",
    "      \n",
    "    self.model = torch.hub.load('ultralytics/yolov5', 'custom', path='./models/best.pt', force_reload=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening on 127.0.0.1:5555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\tiany/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-12-1 Python-3.10.6 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "osu_env = SimOsuEnvironment(max_notes=8, stacked_frames=4, render_mode=False)\n",
    "ac_net = AC_Net(osu_env.observation_space.shape, osu_env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ac_net.load_state_dict(torch.load('models/ac_net.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"C:\\\\tmp\\\\debug_output.txt\", \"w\") as file:\n",
    "#  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimOsuEnvironment' object has no attribute 'num_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m max_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(ac_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mA2C_Agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mac_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mosu_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#total_rewards = agent.train()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiany\\Desktop\\Work\\Dev\\UB CLASS\\FALL 2024\\RL\\final project\\a2c_agent.py:48\u001b[0m, in \u001b[0;36mA2C_Agent.__init__\u001b[1;34m(self, ac_net, env, optimizer, max_episode, behavior_cloning, gamma, beta, grad_clip)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpert_memory \u001b[38;5;241m=\u001b[39m ReplayMemory()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m behavior_cloning:\n\u001b[1;32m---> 48\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_expert_replay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tiany\\Desktop\\Work\\Dev\\UB CLASS\\FALL 2024\\RL\\final project\\a2c_agent.py:161\u001b[0m, in \u001b[0;36mA2C_Agent._get_expert_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    160\u001b[0m   replay \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_name, file))\n\u001b[1;32m--> 161\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_expert_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tiany\\Desktop\\Work\\Dev\\UB CLASS\\FALL 2024\\RL\\final project\\a2c_agent.py:169\u001b[0m, in \u001b[0;36mA2C_Agent._update_expert_memory\u001b[1;34m(self, replay)\u001b[0m\n\u001b[0;32m    166\u001b[0m hit_types \u001b[38;5;241m=\u001b[39m replay[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhit_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    168\u001b[0m note_vector \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mmax_notes\n\u001b[1;32m--> 169\u001b[0m state \u001b[38;5;241m=\u001b[39m deque([note_vector] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_frame\u001b[49m, maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_frame)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(frames)):\n\u001b[0;32m    172\u001b[0m   note_vector \u001b[38;5;241m=\u001b[39m frames[i][:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mmax_notes]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SimOsuEnvironment' object has no attribute 'num_frame'"
     ]
    }
   ],
   "source": [
    "max_episode = 100\n",
    "optimizer = torch.optim.Adam(ac_net.parameters(), lr=0.001)\n",
    "agent = A2C_Agent(ac_net, osu_env, optimizer, max_episode)\n",
    "\n",
    "#total_rewards = agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_rewards = agent.test(ac_net, osu_env, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ac_net.state_dict(), 'models/ac_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(total_rewards)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
